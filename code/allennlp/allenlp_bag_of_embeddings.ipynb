{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from typing import Dict, Iterable, List, Tuple\n",
    "\n",
    "import allennlp\n",
    "import torch\n",
    "from allennlp.data import DataLoader, DatasetReader, Instance, Vocabulary\n",
    "from allennlp.data.fields import LabelField, TextField\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, WhitespaceTokenizer\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules import TextFieldEmbedder, Seq2VecEncoder\n",
    "from allennlp.modules.seq2vec_encoders import BagOfEmbeddingsEncoder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.nn import util\n",
    "from allennlp.training.trainer import GradientDescentTrainer, Trainer\n",
    "from allennlp.training.optimizers import AdamOptimizer\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "\n",
    "from os.path import join as pathjoin\n",
    "import pandas as pd\n",
    "from allennlp.predictors import TextClassifierPredictor\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/mlepekhin/data'\n",
    "MODELS_DIR = '/home/mlepekhin/models'\n",
    "MODEL_ID = 'allennlp_simple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDatasetReader(DatasetReader):\n",
    "    def __init__(self,\n",
    "                 lazy: bool = False,\n",
    "                 tokenizer: Tokenizer = None,\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                 max_tokens: int = None):\n",
    "        super().__init__(lazy)\n",
    "        self.tokenizer = tokenizer or WhitespaceTokenizer()\n",
    "        self.token_indexers = token_indexers or {'tokens': SingleIdTokenIndexer()}\n",
    "        self.max_tokens = max_tokens\n",
    "        \n",
    "    def text_to_instance(self, strings: List[str], label: str = None) -> Instance:\n",
    "        tokens = self.tokenizer.tokenize(strings)\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"text\": sentence_field}\n",
    "        if label is not None:\n",
    "            fields[\"label\"] = LabelField(label)\n",
    "        return Instance(fields)\n",
    "\n",
    "    def _read(self, file_path: str) -> Iterable[Instance]:\n",
    "        dataset_df = pd.read_csv(file_path)\n",
    "        for text, label in zip(dataset_df['text'], dataset_df['target']):\n",
    "            tokens = self.tokenizer.tokenize(text)\n",
    "            text_field = TextField(tokens, self.token_indexers)\n",
    "            yield Instance({'text': text_field, 'label': LabelField(label)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(Model):\n",
    "    def __init__(self,\n",
    "                 vocab: Vocabulary,\n",
    "                 embedder: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder):\n",
    "        super().__init__(vocab)\n",
    "        self.embedder = embedder\n",
    "        self.encoder = encoder\n",
    "        num_labels = vocab.get_vocab_size(\"labels\")\n",
    "        self.classifier = torch.nn.Linear(encoder.get_output_dim(), num_labels)\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "\n",
    "    def forward(self,\n",
    "                text: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor=None) -> Dict[str, torch.Tensor]:\n",
    "        # Shape: (batch_size, num_tokens, embedding_dim)\n",
    "        embedded_text = self.embedder(text)\n",
    "        # Shape: (batch_size, num_tokens)\n",
    "        mask = util.get_text_field_mask(text)\n",
    "        # Shape: (batch_size, encoding_dim)\n",
    "        encoded_text = self.encoder(embedded_text, mask)\n",
    "        # Shape: (batch_size, num_labels)\n",
    "        logits = self.classifier(encoded_text)\n",
    "        # Shape: (batch_size, num_labels)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        if label is not None:\n",
    "            # Shape: (1,)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, label)\n",
    "            self.accuracy(logits, label)\n",
    "            return {'loss': loss, 'probs': probs}\n",
    "        else:\n",
    "            return {'probs': probs}\n",
    "    \n",
    "    def get_metrics(self, reset: bool = True) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}\n",
    "\n",
    "\n",
    "def build_dataset_reader() -> DatasetReader:\n",
    "    return ClassificationDatasetReader()\n",
    "\n",
    "\n",
    "def read_data(reader: DatasetReader) -> Tuple[Iterable[Instance], Iterable[Instance]]:\n",
    "    print(\"Reading data\")\n",
    "    training_data = reader.read(pathjoin(DATA_DIR, \"multi_train\"))\n",
    "    validation_data = reader.read(pathjoin(DATA_DIR, \"multi_test\"))\n",
    "    return training_data, validation_data\n",
    "\n",
    "\n",
    "def build_vocab(instances: Iterable[Instance]) -> Vocabulary:\n",
    "    print(\"Building the vocabulary\")\n",
    "    return Vocabulary.from_instances(instances)\n",
    "\n",
    "\n",
    "def build_model(vocab: Vocabulary) -> Model:\n",
    "    print(\"Building the model\")\n",
    "    vocab_size = vocab.get_vocab_size(\"tokens\")\n",
    "    embedder = BasicTextFieldEmbedder(\n",
    "        {\"tokens\": Embedding(embedding_dim=10, num_embeddings=vocab_size)})\n",
    "    encoder = BagOfEmbeddingsEncoder(embedding_dim=10)\n",
    "    return SimpleClassifier(vocab, embedder, encoder)\n",
    "\n",
    "\n",
    "def run_training_loop():\n",
    "    dataset_reader = build_dataset_reader()\n",
    "\n",
    "    # These are a subclass of pytorch Datasets, with some allennlp-specific\n",
    "    # functionality added.\n",
    "    train_data, dev_data = read_data(dataset_reader)\n",
    "\n",
    "    vocab = build_vocab(train_data + dev_data)\n",
    "    model = build_model(vocab)\n",
    "\n",
    "    # This is the allennlp-specific functionality in the Dataset object;\n",
    "    # we need to be able convert strings in the data to integers, and this\n",
    "    # is how we do it.\n",
    "    train_data.index_with(vocab)\n",
    "    dev_data.index_with(vocab)\n",
    "\n",
    "    # These are again a subclass of pytorch DataLoaders, with an\n",
    "    # allennlp-specific collate function, that runs our indexing and\n",
    "    # batching code.\n",
    "    train_loader, dev_loader = build_data_loaders(train_data, dev_data)\n",
    "\n",
    "    # You obviously won't want to create a temporary file for your training\n",
    "    # results, but for execution in binder for this course, we need to do this.\n",
    "    with tempfile.TemporaryDirectory() as serialization_dir:\n",
    "        trainer = build_trainer(\n",
    "            model,\n",
    "            serialization_dir,\n",
    "            train_loader,\n",
    "            dev_loader\n",
    "        )\n",
    "        print(\"Starting training\")\n",
    "        trainer.train()\n",
    "        print(\"Finished training\")\n",
    "    return trainer\n",
    "\n",
    "\n",
    "# The other `build_*` methods are things we've seen before, so they are\n",
    "# in the setup section above.\n",
    "def build_data_loaders(\n",
    "    train_data: torch.utils.data.Dataset,\n",
    "    dev_data: torch.utils.data.Dataset,\n",
    ") -> Tuple[allennlp.data.DataLoader, allennlp.data.DataLoader]:\n",
    "    # Note that DataLoader is imported from allennlp above, *not* torch.\n",
    "    # We need to get the allennlp-specific collate function, which is\n",
    "    # what actually does indexing and batching.\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    dev_loader = DataLoader(dev_data, batch_size=32, shuffle=False)\n",
    "    return train_loader, dev_loader\n",
    "\n",
    "\n",
    "def build_trainer(\n",
    "    model: Model,\n",
    "    serialization_dir: str,\n",
    "    train_loader: DataLoader,\n",
    "    dev_loader: DataLoader,\n",
    "    num_epochs: int = 1\n",
    ") -> Trainer:\n",
    "    parameters = [\n",
    "        [n, p]\n",
    "        for n, p in model.named_parameters() if p.requires_grad\n",
    "    ]\n",
    "    optimizer = AdamOptimizer(parameters)\n",
    "    trainer = GradientDescentTrainer(\n",
    "        model=model,\n",
    "        serialization_dir=serialization_dir,\n",
    "        data_loader=train_loader,\n",
    "        validation_data_loader=dev_loader,\n",
    "        num_epochs=num_epochs,\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c7555de4c04d85a54c964b8694e745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdfba66f03447779a0398b962f087ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building the vocabulary\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2829a2685f6f4469b9b36a2556264aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3616.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#trainer = run_training_loop()\n",
    "'''\n",
    "dataset_reader = build_dataset_reader()\n",
    "\n",
    "train_data, dev_data = read_data(dataset_reader)\n",
    "\n",
    "vocab = build_vocab(train_data + dev_data)\n",
    "\n",
    "train_data.index_with(vocab)\n",
    "dev_data.index_with(vocab)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    cuda_device = 0\n",
    "    model = model.cuda(cuda_device)\n",
    "else:\n",
    "    cuda_device = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe694f9707546edb014b40127d0598b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=85.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-90315843d451>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlepekhin_research/lib/python3.8/site-packages/allennlp/training/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m             \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m             \u001b[0;31m# get peak of memory usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlepekhin_research/lib/python3.8/site-packages/allennlp/training/trainer.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mbatch_group_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_group\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m                 \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfor_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mbatch_group_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlepekhin_research/lib/python3.8/site-packages/allennlp/training/trainer.py\u001b[0m in \u001b[0;36mbatch_outputs\u001b[0;34m(self, batch, for_training)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \"\"\"\n\u001b[1;32m    461\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytorch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfor_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlepekhin_research/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-887bcb1300f9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, label)\u001b[0m\n\u001b[1;32m     15\u001b[0m                 label: torch.Tensor=None) -> Dict[str, torch.Tensor]:\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Shape: (batch_size, num_tokens, embedding_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0membedded_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Shape: (batch_size, num_tokens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_field_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlepekhin_research/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlepekhin_research/lib/python3.8/site-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text_field_input, num_wrapping_dims, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;31m# If there's only one tensor argument to the embedder, and we just have one tensor to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;31m# embed, we can just pass in that tensor, without requiring a name match.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mtoken_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;31m# If there are multiple tensor arguments, we have to require matching names from the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlepekhin_research/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlepekhin_research/lib/python3.8/site-packages/allennlp/modules/token_embedders/embedding.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_initial_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         embedded = embedding(\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlepekhin_research/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1722\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select"
     ]
    }
   ],
   "source": [
    "train_loader, dev_loader = build_data_loaders(train_data, dev_data)\n",
    "\n",
    "# You obviously won't want to create a temporary file for your training\n",
    "# results, but for execution in binder for this course, we need to do this.\n",
    "with tempfile.TemporaryDirectory() as serialization_dir:\n",
    "    trainer = build_trainer(\n",
    "        model,\n",
    "        serialization_dir,\n",
    "        train_loader,\n",
    "        dev_loader,\n",
    "        5\n",
    "    )\n",
    "    print(\"Starting training\")\n",
    "    trainer.train()\n",
    "    print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(model.state_dict, pathjoin(MODELS_DIR, MODEL_ID, 'model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.save_to_files(pathjoin(MODELS_DIR, MODEL_ID, 'vocab'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A1', 'A8', 'A12', 'A14', 'A16', 'A7', 'A11', 'A17', 'A4', 'A9', 'A22']\n"
     ]
    }
   ],
   "source": [
    "id_to_label = []\n",
    "with open(pathjoin(MODELS_DIR, MODEL_ID, 'vocab', 'labels.txt')) as vocab_in:\n",
    "    for line in vocab_in:\n",
    "        id_to_label.append(line.strip())\n",
    "print(id_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classes(sentence_list):\n",
    "    predictor = TextClassifierPredictor(model, dataset_reader=build_dataset_reader())\n",
    "    result = [id_to_label[np.argmax(predictor.predict(sentence)[\"probs\"])]\\\n",
    "              for sentence in sentence_list]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A1', 'A8', 'A1', 'A11']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_classes(['Здесь должно быть ваше сообщение',\n",
    "                 'Коты - лучшие домашние животные. К такому выводу пришли эксперты из издания New York Times',\n",
    "                 'It is no more than what it is.',\n",
    "                 'Жила я как-то с парнем. Я только вот на днях уволилась с работы, так как мне тяжело было работать сутки через сутки, должна была выходить на другую работу. И именно в этот период, мне сильно поплохело, начались жуткие головные боли, слабость, обмороки. Парень настоял, что нужно срочно вызывать врача. Приехала скорая, фельдшер мужик лет 50 весь седой. Позадавал вопросы мне, где болит, как болит, и зачем болит? Кто такая вообще по жизни, и чем занимаюсь? Смерил давление, температуру, написал что-то в своих бумагах, и дав лишь рекомендацию: \"больше отдыхайте, пейте воду, гуляйте на свежем воздухе\" пошёл на выход.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the time to interpret our simple classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.interpret.saliency_interpreters import SmoothGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_grad_interpr = SmoothGradient(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instance_1': {'grad_input_1': [0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602,\n",
       "   0.010204082900614602]}}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smooth_grad_interpr.saliency_interpret_from_json({'sentence': 'Жила я как-то с парнем. Я только вот на днях уволилась с работы, так как мне тяжело было работать сутки через сутки, должна была выходить на другую работу. И именно в этот период, мне сильно поплохело, начались жуткие головные боли, слабость, обмороки. Парень настоял, что нужно срочно вызывать врача. Приехала скорая, фельдшер мужик лет 50 весь седой. Позадавал вопросы мне, где болит, как болит, и зачем болит? Кто такая вообще по жизни, и чем занимаюсь? Смерил давление, температуру, написал что-то в своих бумагах, и дав лишь рекомендацию: \"больше отдыхайте, пейте воду, гуляйте на свежем воздухе\" пошёл на выход.'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
