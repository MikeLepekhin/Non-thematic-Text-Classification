{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from typing import Dict, Iterable, List, Tuple\n",
    "\n",
    "import allennlp\n",
    "import torch\n",
    "from allennlp.data import DataLoader, DatasetReader, Instance, Vocabulary\n",
    "from allennlp.data.fields import LabelField, TextField\n",
    "from allennlp.data.token_indexers import TokenIndexer, PretrainedTransformerIndexer\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, PretrainedTransformerTokenizer\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules import TextFieldEmbedder, Seq2VecEncoder\n",
    "from allennlp.modules.token_embedders import PretrainedTransformerEmbedder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.seq2vec_encoders import BertPooler\n",
    "from allennlp.nn import util\n",
    "from allennlp.training.trainer import GradientDescentTrainer, Trainer\n",
    "from allennlp.training.optimizers import AdamOptimizer\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "\n",
    "from os.path import join as pathjoin\n",
    "import pandas as pd\n",
    "from allennlp.predictors import TextClassifierPredictor\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/mlepekhin/data'\n",
    "MODELS_DIR = '/home/mlepekhin/models'\n",
    "MODEL_ID = 'allennlp_rubert'\n",
    "CHECKPOINTS_DIR = pathjoin(MODELS_DIR, MODEL_ID, 'checkpoints')\n",
    "BEST_MODEL = pathjoin(CHECKPOINTS_DIR, 'best.th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = 'DeepPavlov/rubert-base-cased'\n",
    "MAX_TOKENS = 510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(Model):\n",
    "    def __init__(self,\n",
    "                 vocab: Vocabulary,\n",
    "                 embedder: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder):\n",
    "        super().__init__(vocab)\n",
    "        self.embedder = embedder \n",
    "        num_labels = vocab.get_vocab_size(\"labels\")\n",
    "        self.encoder = encoder\n",
    "        self.classifier = torch.nn.Linear(encoder.get_output_dim(), num_labels)\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        \n",
    "\n",
    "    def forward(self,\n",
    "                text: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor=None) -> Dict[str, torch.Tensor]:\n",
    "        # Shape: (batch_size, num_tokens, embedding_dim)\n",
    "        embedded_text = self.embedder(text)\n",
    "        #print(\"embed shape\", embedded_text.shape)\n",
    "        # Shape: (batch_size, num_tokens)\n",
    "        mask = util.get_text_field_mask(text)\n",
    "        #print(\"mask shape\", mask.shape)\n",
    "        # Shape: (batch_size, encoding_dim)\n",
    "        encoded_text = self.encoder(embedded_text, mask)\n",
    "        # Shape: (batch_size, num_labels)\n",
    "        logits = self.classifier(encoded_text)\n",
    "        # Shape: (batch_size, num_labels)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        if label is not None:\n",
    "            loss = torch.nn.functional.cross_entropy(logits, label)\n",
    "            self.accuracy(logits, label)\n",
    "            return {'loss': loss, 'probs': probs}\n",
    "        else:\n",
    "            return {'probs': probs}\n",
    "    \n",
    "    def get_metrics(self, reset: bool = True) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}\n",
    "\n",
    "\n",
    "def read_data(reader: DatasetReader, train_path: str, val_path: str) -> Tuple[Iterable[Instance], Iterable[Instance]]:\n",
    "    print(\"Reading data\")\n",
    "    training_data = reader.read(train_path)\n",
    "    validation_data = reader.read(val_path)\n",
    "    return training_data, validation_data\n",
    "\n",
    "\n",
    "def build_vocab(instances: Iterable[Instance]) -> Vocabulary:\n",
    "    print(\"Building the vocabulary\")\n",
    "    return Vocabulary.from_instances(instances)\n",
    "\n",
    "\n",
    "def build_model(vocab: Vocabulary) -> Model:\n",
    "    print(\"Building the model\")\n",
    "    vocab_size = vocab.get_vocab_size(\"tokens\")\n",
    "    #embedder = BasicTextFieldEmbedder(\n",
    "    #    {\"tokens\": Embedding(embedding_dim=10, num_embeddings=vocab_size)})\n",
    "    embedding = PretrainedTransformerEmbedder(model_name=transformer_model)\n",
    "    embedder = BasicTextFieldEmbedder(token_embedders={'bert_tokens': embedding})\n",
    "    encoder = BertPooler(transformer_model)\n",
    "    return SimpleClassifier(vocab, embedder, encoder)\n",
    "\n",
    "def build_dataset_reader() -> DatasetReader:\n",
    "    return ClassificationDatasetReader()\n",
    "\n",
    "def run_training_loop():\n",
    "    dataset_reader = build_dataset_reader()\n",
    "\n",
    "    # These are a subclass of pytorch Datasets, with some allennlp-specific\n",
    "    # functionality added.\n",
    "    train_data, dev_data = read_data(dataset_reader)\n",
    "\n",
    "    vocab = build_vocab(train_data + dev_data)\n",
    "    model = build_model(vocab)\n",
    "\n",
    "    # This is the allennlp-specific functionality in the Dataset object;\n",
    "    # we need to be able convert strings in the data to integers, and this\n",
    "    # is how we do it.\n",
    "    train_data.index_with(vocab)\n",
    "    dev_data.index_with(vocab)\n",
    "\n",
    "    # These are again a subclass of pytorch DataLoaders, with an\n",
    "    # allennlp-specific collate function, that runs our indexing and\n",
    "    # batching code.\n",
    "    train_loader, dev_loader = build_data_loaders(train_data, dev_data)\n",
    "\n",
    "    # You obviously won't want to create a temporary file for your training\n",
    "    # results, but for execution in binder for this course, we need to do this.\n",
    "    with tempfile.TemporaryDirectory() as serialization_dir:\n",
    "        trainer = build_trainer(\n",
    "            model,\n",
    "            serialization_dir,\n",
    "            train_loader,\n",
    "            dev_loader\n",
    "        )\n",
    "        print(\"Starting training\")\n",
    "        trainer.train()\n",
    "        print(\"Finished training\")\n",
    "    return trainer\n",
    "\n",
    "\n",
    "# The other `build_*` methods are things we've seen before, so they are\n",
    "# in the setup section above.\n",
    "def build_data_loaders(\n",
    "    train_data: torch.utils.data.Dataset,\n",
    "    dev_data: torch.utils.data.Dataset,\n",
    ") -> Tuple[allennlp.data.DataLoader, allennlp.data.DataLoader]:\n",
    "    # Note that DataLoader is imported from allennlp above, *not* torch.\n",
    "    # We need to get the allennlp-specific collate function, which is\n",
    "    # what actually does indexing and batching.\n",
    "    train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "    dev_loader = DataLoader(dev_data, batch_size=16, shuffle=False)\n",
    "    return train_loader, dev_loader\n",
    "\n",
    "\n",
    "def build_trainer(\n",
    "    model: Model,\n",
    "    serialization_dir: str,\n",
    "    train_loader: DataLoader,\n",
    "    dev_loader: DataLoader,\n",
    "    num_epochs: int = 1,\n",
    "    cuda_device: int = -1\n",
    ") -> Trainer:\n",
    "    parameters = [\n",
    "        [n, p]\n",
    "        for n, p in model.named_parameters() if p.requires_grad\n",
    "    ]\n",
    "    optimizer = AdamOptimizer(parameters, lr=0.000025)\n",
    "    trainer = GradientDescentTrainer(\n",
    "        model=model,\n",
    "        serialization_dir=serialization_dir,\n",
    "        data_loader=train_loader,\n",
    "        validation_data_loader=dev_loader,\n",
    "        num_epochs=num_epochs,\n",
    "        optimizer=optimizer,\n",
    "        cuda_device=cuda_device,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer tokens: [[CLS], hel, ##lo, !, i, ', s, no, more, than, just, my, op, ##ini, ##on, ., but, i, want, ##ed, to, say, you, that, i, don, ', t, car, ##e, [SEP]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PretrainedTransformerTokenizer(model_name=transformer_model)\n",
    "token_indexer = PretrainedTransformerIndexer(model_name=transformer_model)\n",
    "text = \"Hello! I's no more than just my opinion. But I wanted to say you that I don't care\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Transformer tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.interpret.saliency_interpreters import SmoothGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDatasetReader(DatasetReader):\n",
    "    def __init__(self,\n",
    "                 lazy: bool = False,\n",
    "                 tokenizer: Tokenizer = None,\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                 max_tokens: int = None):\n",
    "        super().__init__(lazy)\n",
    "        self.tokenizer = tokenizer or PretrainedTransformerTokenizer(transformer_model, max_length=MAX_TOKENS)\n",
    "        self.token_indexers = token_indexers or {'bert_tokens': PretrainedTransformerIndexer(transformer_model)}\n",
    "        self.max_tokens = max_tokens\n",
    "        \n",
    "    def text_to_instance(self, string: str, label: str = None) -> Instance:\n",
    "        tokens = self.tokenizer.tokenize(string)\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"text\": sentence_field}\n",
    "        if label is not None:\n",
    "            fields[\"label\"] = LabelField(label)\n",
    "        return Instance(fields)\n",
    "\n",
    "    def _read(self, file_path: str) -> Iterable[Instance]:\n",
    "        dataset_df = pd.read_csv(file_path)\n",
    "        for text, label in zip(dataset_df['text'], dataset_df['target']):\n",
    "            yield self.text_to_instance(text, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(Model):\n",
    "    def __init__(self,\n",
    "                 vocab: Vocabulary,\n",
    "                 embedder: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder):\n",
    "        super().__init__(vocab)\n",
    "        self.embedder = embedder \n",
    "        #num_labels = vocab.get_vocab_size(\"labels\")\n",
    "        num_labels=10\n",
    "        self.encoder = encoder\n",
    "        self.classifier = torch.nn.Linear(encoder.get_output_dim(), num_labels)\n",
    "        print(encoder.get_output_dim(), num_labels)\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        \n",
    "\n",
    "    def forward(self,\n",
    "                text: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor=None) -> Dict[str, torch.Tensor]:\n",
    "        # Shape: (batch_size, num_tokens, embedding_dim)\n",
    "        embedded_text = self.embedder(text)\n",
    "        #print(\"embed shape\", embedded_text.shape)\n",
    "        # Shape: (batch_size, num_tokens)\n",
    "        mask = util.get_text_field_mask(text)\n",
    "        #print(\"mask shape\", mask.shape)\n",
    "        # Shape: (batch_size, encoding_dim)\n",
    "        encoded_text = self.encoder(embedded_text, mask)\n",
    "        # Shape: (batch_size, num_labels)\n",
    "        logits = self.classifier(encoded_text)\n",
    "        # Shape: (batch_size, num_labels)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        if label is not None:\n",
    "            loss = torch.nn.functional.cross_entropy(logits, label)\n",
    "            self.accuracy(logits, label)\n",
    "            return {'loss': loss, 'probs': probs}\n",
    "        else:\n",
    "            return {'probs': probs}\n",
    "    \n",
    "    def get_metrics(self, reset: bool = True) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocabulary with namespaces:  tags, Size: 119547 || labels, Size: 10 || Non Padded Namespaces: {'*labels', '*tags'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocabulary()\n",
    "vocab.from_files(pathjoin(MODELS_DIR, MODEL_ID, 'vocab'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(vocab.get_token_to_index_vocabulary('labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    cuda_device = 0\n",
    "else:\n",
    "    cuda_device = -1\n",
    "print(cuda_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model\n",
      "768 10\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab).cuda(cuda_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(BEST_MODEL, map_location=f'cuda:{cuda_device}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>726</td>\n",
       "      <td>A7</td>\n",
       "      <td>Глава 1 Приступая к работе 1.1 Знакомство с те...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1871</td>\n",
       "      <td>A17</td>\n",
       "      <td>Kawasaki D-Tracker С недавних пор Kawasaki d-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1265</td>\n",
       "      <td>A17</td>\n",
       "      <td>По моему , вполне достойные книги , может и не...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>205</td>\n",
       "      <td>A11</td>\n",
       "      <td>Тест-драйв Lada Granta : новая надежда автогра...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>141</td>\n",
       "      <td>A8</td>\n",
       "      <td>среда , 2 декабря 2009 года , 12.33 Бумага всё...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 target                                               text\n",
       "0         726     A7  Глава 1 Приступая к работе 1.1 Знакомство с те...\n",
       "1        1871    A17  Kawasaki D-Tracker С недавних пор Kawasaki d-t...\n",
       "2        1265    A17  По моему , вполне достойные книги , может и не...\n",
       "3         205    A11  Тест-драйв Lada Granta : новая надежда автогра...\n",
       "4         141     A8  среда , 2 декабря 2009 года , 12.33 Бумага всё..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_test_df = pd.read_csv(pathjoin(DATA_DIR, 'ru_test'))\n",
    "ru_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_classes = ru_test_df.target.values\n",
    "sentences = ru_test_df.text.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_description = {\n",
    "    'A1': 'argum',\n",
    "    'A3': 'emotive',\n",
    "    'A4': 'fictive',\n",
    "    'A5': 'flippant',\n",
    "    'A6': 'informal',\n",
    "    'A7': 'instruct',\n",
    "    'A8': 'reporting',\n",
    "    'A9': 'legal',\n",
    "    'A11': 'personal',\n",
    "    'A12': 'commercial',\n",
    "    'A13': 'propaganda',\n",
    "    'A14': 'research',\n",
    "    'A15': 'specialist',\n",
    "    'A16': 'info',\n",
    "    'A17': 'eval',\n",
    "    'A19': 'poetic',\n",
    "    'A20': 'appeal',\n",
    "    'A22': 'stuff'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TextClassifierPredictor(model, dataset_reader=build_dataset_reader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classes(sentence_list):\n",
    "    return [id_to_label[np.argmax(predictor.predict(sentence)['probs'])] for sentence in sentence_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = np.array(predict_classes(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label (argum) f1_score 0.09411764705882353 precision 0.05194805194805195 recall 0.5\n",
      "label (fictive) f1_score 0.6 precision 0.42857142857142855 recall 1.0\n",
      "label (instruct) f1_score 0.3614457831325301 precision 0.3 recall 0.45454545454545453\n",
      "label (reporting) f1_score 0.1 precision 0.05405405405405406 recall 0.6666666666666666\n",
      "label (legal) f1_score 0.0 precision 0.0 recall 0.0\n",
      "label (personal) f1_score 0.3076923076923077 precision 0.42105263157894735 recall 0.24242424242424243\n",
      "label (commercial) f1_score 0.11267605633802819 precision 0.06060606060606061 recall 0.8\n",
      "label (research) f1_score 0.13256484149855907 precision 1.0 recall 0.07098765432098765\n",
      "label (info) f1_score 0.0 precision 0.0 recall 0.0\n",
      "label (eval) f1_score 0.0 precision 0.0 recall 0.0\n",
      "accuracy 0.16113744075829384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlepekhin/anaconda3/envs/mlepekhin_research/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for label, description in label_description.items():\n",
    "    true_binary = true_classes == label\n",
    "    if np.sum(true_binary) == 0:\n",
    "        continue\n",
    "    predicted_binary = predicted_classes == label\n",
    "    print(\n",
    "        f\"label ({description})\", \n",
    "        f\"f1_score {f1_score(predicted_binary, true_binary)}\", \n",
    "        f\"precision {precision_score(predicted_binary, true_binary)}\", \n",
    "        f\"recall {recall_score(predicted_binary, true_binary)}\", \n",
    "    )\n",
    "print(f\"accuracy\", accuracy_score(predicted_classes, true_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth Gradient Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_grad_interpr = SmoothGradient(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
