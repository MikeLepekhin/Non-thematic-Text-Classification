{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pathjoin\n",
    "from data_processing import *\n",
    "from interpretation import *\n",
    "from models import *\n",
    "from training import *\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/mlepekhin/data'\n",
    "MODELS_DIR = '/home/mlepekhin/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = 'bert-base-cased'\n",
    "MAX_TOKENS = 512\n",
    "\n",
    "BERT_MODEL_ID = 'allennlp_bert_base_cased'\n",
    "CNN_MODEL_ID = 'allennlp_simple_cnn_en'\n",
    "\n",
    "BERT_BEST_MODEL = pathjoin(MODELS_DIR, BERT_MODEL_ID, 'checkpoints', 'best.th')\n",
    "CNN_BEST_MODEL = pathjoin(MODELS_DIR, CNN_MODEL_ID, 'checkpoints', 'best.th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary().from_files(pathjoin(MODELS_DIR, BERT_MODEL_ID, 'vocab'))\n",
    "model = build_transformer_model(vocab, transformer_model)\n",
    "model.load_state_dict(torch.load(BERT_BEST_MODEL))\n",
    "\n",
    "bert_predictor = TextClassifierPredictor(\n",
    "    model, \n",
    "    dataset_reader=build_transformer_dataset_reader(transformer_model, lower=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model\n"
     ]
    }
   ],
   "source": [
    "cnn_vocab = Vocabulary().from_files(pathjoin(MODELS_DIR, CNN_MODEL_ID, 'vocab'))\n",
    "cnn_model = build_simple_cnn_model(\n",
    "    cnn_vocab, emb_size=256, output_dim=128, num_filters=32, ngram_filter_sizes=(2, 3, 4, 5)\n",
    ")\n",
    "cnn_model.load_state_dict(torch.load(CNN_BEST_MODEL))\n",
    "\n",
    "cnn_predictor = TextClassifierPredictor(\n",
    "    cnn_model, \n",
    "    dataset_reader=build_dataset_reader(None, lower=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>726</td>\n",
       "      <td>A7</td>\n",
       "      <td>Глава 1 Приступая к работе 1.1 Знакомство с те...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1871</td>\n",
       "      <td>A17</td>\n",
       "      <td>Kawasaki D-Tracker С недавних пор Kawasaki d-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1265</td>\n",
       "      <td>A17</td>\n",
       "      <td>По моему , вполне достойные книги , может и не...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>205</td>\n",
       "      <td>A11</td>\n",
       "      <td>Тест-драйв Lada Granta : новая надежда автогра...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>141</td>\n",
       "      <td>A8</td>\n",
       "      <td>среда , 2 декабря 2009 года , 12.33 Бумага всё...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 target                                               text\n",
       "0         726     A7  Глава 1 Приступая к работе 1.1 Знакомство с те...\n",
       "1        1871    A17  Kawasaki D-Tracker С недавних пор Kawasaki d-t...\n",
       "2        1265    A17  По моему , вполне достойные книги , может и не...\n",
       "3         205    A11  Тест-драйв Lada Granta : новая надежда автогра...\n",
       "4         141     A8  среда , 2 декабря 2009 года , 12.33 Бумага всё..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(pathjoin(DATA_DIR, 'ru_test'))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'A1', 1: 'A12', 2: 'A7', 3: 'A16', 4: 'A8', 5: 'A22', 6: 'A4', 7: 'A11', 8: 'A14', 9: 'A9', 10: 'A17'}\n",
      "{0: 'A1', 1: 'A12', 2: 'A7', 3: 'A16', 4: 'A8', 5: 'A22', 6: 'A4', 7: 'A11', 8: 'A14', 9: 'A9', 10: 'A17'}\n"
     ]
    }
   ],
   "source": [
    "index_to_token = vocab.get_index_to_token_vocabulary('labels')\n",
    "token_to_index = vocab.get_token_to_index_vocabulary('labels')\n",
    "print(index_to_token)\n",
    "print(cnn_vocab.get_index_to_token_vocabulary('labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "483it [02:10,  3.72it/s]\n"
     ]
    }
   ],
   "source": [
    "probs_bert, probs_cnn = [], []\n",
    "\n",
    "for text, target in tqdm.tqdm(zip(test_df.text.values, test_df.target.values)):\n",
    "    probs_bert.append(bert_predictor.predict(text)['probs'])\n",
    "    probs_cnn.append(cnn_predictor.predict(text)['probs'])\n",
    "probs_bert = np.array(probs_bert)\n",
    "probs_cnn = np.array(probs_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_mix = 0.5 * probs_bert + 0.5 * probs_cnn\n",
    "predicted_bert = [index_to_token[np.argmax(vec)] for vec in probs_bert]\n",
    "predicted_cnn = [index_to_token[np.argmax(vec)] for vec in probs_cnn]\n",
    "predicted_mix = [index_to_token[np.argmax(vec)] for vec in probs_mix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label (argum) f1_score 0.0 precision 0.0 recall 0.0\n",
      "label (fictive) f1_score 0.0 precision 0.0 recall 0.0\n",
      "label (instruct) f1_score 0.0 precision 0.0 recall 0.0\n",
      "label (reporting) f1_score 0.0 precision 0.0 recall 0.0\n",
      "label (legal) f1_score 0.0 precision 0.0 recall 0.0\n",
      "label (personal) f1_score 0.0 precision 0.0 recall 0.0\n",
      "label (commercial) f1_score 0.0 precision 0.0 recall 0.0\n",
      "label (research) f1_score 0.0 precision 0.0 recall 0.0\n",
      "label (info) f1_score 0.07142857142857144 precision 0.06060606060606061 recall 0.08695652173913043\n",
      "label (eval) f1_score 0.0 precision 0.0 recall 0.0\n",
      "accuracy 0.004140786749482402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlepekhin/anaconda3/envs/mlepekhin_research/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "calc_classifier_metrics(np.array(predicted_bert), test_df.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label (argum) f1_score 0.07407407407407407 precision 0.03896103896103896 recall 0.75\n",
      "label (fictive) f1_score 0.0 precision 0.0 recall 0.0\n",
      "label (instruct) f1_score 0.0 precision 0.0 recall 0.0\n",
      "label (reporting) f1_score 0.3956043956043956 precision 0.5242718446601942 recall 0.3176470588235294\n",
      "label (legal) f1_score 0.0 precision 0.0 recall 0.0\n",
      "label (personal) f1_score 0.0 precision 0.0 recall 0.0\n",
      "label (commercial) f1_score 0.22058823529411767 precision 0.17647058823529413 recall 0.29411764705882354\n",
      "label (research) f1_score 0.09836065573770492 precision 0.061224489795918366 recall 0.25\n",
      "label (info) f1_score 0.16727272727272727 precision 0.696969696969697 recall 0.09504132231404959\n",
      "label (eval) f1_score 0.0 precision 0.0 recall 0.0\n",
      "accuracy 0.2028985507246377\n"
     ]
    }
   ],
   "source": [
    "calc_classifier_metrics(np.array(predicted_cnn), test_df.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label (argum) f1_score 0.0 precision 0.0 recall 0.0\n",
      "label (fictive) f1_score 0.0 precision 0.0 recall 0.0\n",
      "label (instruct) f1_score 0.0 precision 0.0 recall 0.0\n",
      "label (reporting) f1_score 0.0 precision 0.0 recall 0.0\n",
      "label (legal) f1_score 0.0 precision 0.0 recall 0.0\n",
      "label (personal) f1_score 0.0 precision 0.0 recall 0.0\n",
      "label (commercial) f1_score 0.0 precision 0.0 recall 0.0\n",
      "label (research) f1_score 0.0 precision 0.0 recall 0.0\n",
      "label (info) f1_score 0.0967741935483871 precision 0.09090909090909091 recall 0.10344827586206896\n",
      "label (eval) f1_score 0.0 precision 0.0 recall 0.0\n",
      "accuracy 0.006211180124223602\n"
     ]
    }
   ],
   "source": [
    "calc_classifier_metrics(np.array(predicted_mix), test_df.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os.path import join as pathjoin\n",
    "\n",
    "def save_model(predictor, vectorizer, model_dir):\n",
    "    !mkdir {model_dir}\n",
    "    with open(pathjoin(model_dir, 'predictor'), 'wb') as fout:\n",
    "        fout.write(pickle.dumps(predictor))\n",
    "    with open(pathjoin(model_dir, 'vectorizer'), 'wb') as fout:\n",
    "        fout.write(pickle.dumps(vectorizer))\n",
    "        \n",
    "def load_model(model_dir):\n",
    "    return pickle.loads(open(pathjoin(model_dir, 'predictor'), 'rb').read()),\\\n",
    "           pickle.loads(open(pathjoin(model_dir, 'vectorizer'), 'rb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "class BigVectorizer:\n",
    "    def __init__(self, max_word_features=5000, max_char_features=10000):\n",
    "        self.vect_word = TfidfVectorizer(\n",
    "            max_features=max_word_features, lowercase=True, analyzer='word',\n",
    "            stop_words=stopwords.words('russian'), ngram_range=(1,3),dtype=np.float32\n",
    "        )\n",
    "        self.vect_char = TfidfVectorizer(\n",
    "            max_features=max_char_features, lowercase=True, analyzer='char',\n",
    "            stop_words=stopwords.words('russian'), ngram_range=(3,6),dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        vect_word = self.vect_word.fit_transform(X)\n",
    "        vect_char = self.vect_char.fit_transform(X)\n",
    "        return sparse.hstack([vect_word, vect_char])\n",
    "       \n",
    "    def transform(self, X):\n",
    "        vect_word = self.vect_word.transform(X)\n",
    "        vect_char = self.vect_char.transform(X)\n",
    "        return sparse.hstack([vect_word, vect_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictor, vectorizer = load_model('simple_lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_proba = lr_predictor.predict_proba(vectorizer.transform(test_df.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_lr = lr_predictor.predict(vectorizer.transform(test_df.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label (argum) f1_score 0.7100591715976331 precision 0.7792207792207793 recall 0.6521739130434783\n",
      "label (fictive) f1_score 0.7567567567567568 precision 0.6086956521739131 recall 1.0\n",
      "label (instruct) f1_score 0.6923076923076924 precision 0.5294117647058824 recall 1.0\n",
      "label (reporting) f1_score 0.9124423963133641 precision 0.9611650485436893 recall 0.868421052631579\n",
      "label (legal) f1_score 0.9230769230769231 precision 0.9230769230769231 recall 0.9230769230769231\n",
      "label (personal) f1_score 0.6732673267326732 precision 0.6938775510204082 recall 0.6538461538461539\n",
      "label (commercial) f1_score 0.9239766081871345 precision 0.9294117647058824 recall 0.9186046511627907\n",
      "label (research) f1_score 0.8200000000000001 precision 0.8367346938775511 recall 0.803921568627451\n",
      "label (info) f1_score 0.39999999999999997 precision 0.2727272727272727 recall 0.75\n",
      "label (eval) f1_score 0.7567567567567567 precision 0.8235294117647058 recall 0.7\n",
      "accuracy 0.7971014492753623\n"
     ]
    }
   ],
   "source": [
    "calc_classifier_metrics(np.array(predicted_lr), test_df.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_index = {}\n",
    "for class_id, cl in enumerate(lr_predictor.classes_):\n",
    "    index_to_index[class_id] = token_to_index[cl]\n",
    "\n",
    "lr_proba_transformed = []\n",
    "for vec in lr_proba:\n",
    "    new_vec = np.zeros(len(vec))\n",
    "    for i in range(len(vec)):\n",
    "        new_vec[index_to_index[i]] = vec[i]\n",
    "    lr_proba_transformed.append(new_vec[:])\n",
    "lr_proba_transformed = np.array(lr_proba_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label (argum) f1_score 0.7272727272727272 precision 0.7792207792207793 recall 0.6818181818181818\n",
      "label (fictive) f1_score 0.8292682926829269 precision 0.7391304347826086 recall 0.9444444444444444\n",
      "label (instruct) f1_score 0.8387096774193549 precision 0.7647058823529411 recall 0.9285714285714286\n",
      "label (reporting) f1_score 0.9573459715639812 precision 0.9805825242718447 recall 0.9351851851851852\n",
      "label (legal) f1_score 0.8148148148148148 precision 0.8461538461538461 recall 0.7857142857142857\n",
      "label (personal) f1_score 0.7368421052631581 precision 0.7142857142857143 recall 0.7608695652173914\n",
      "label (commercial) f1_score 0.9404761904761904 precision 0.9294117647058824 recall 0.9518072289156626\n",
      "label (research) f1_score 0.8627450980392157 precision 0.8979591836734694 recall 0.8301886792452831\n",
      "label (info) f1_score 0.5306122448979591 precision 0.3939393939393939 recall 0.8125\n",
      "label (eval) f1_score 0.7532467532467532 precision 0.8529411764705882 recall 0.6744186046511628\n",
      "accuracy 0.8322981366459627\n"
     ]
    }
   ],
   "source": [
    "probs_mix_lr = 0.3 * probs_bert + 0.4 * lr_proba_transformed + 0.3 * probs_cnn\n",
    "predicted_mix_lr = [index_to_token[np.argmax(vec)] for vec in probs_mix_lr]\n",
    "calc_classifier_metrics(np.array(predicted_mix_lr), test_df.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
