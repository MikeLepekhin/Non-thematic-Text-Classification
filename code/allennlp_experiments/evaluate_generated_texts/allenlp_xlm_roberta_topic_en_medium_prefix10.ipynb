{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pathjoin\n",
    "import sys\n",
    "sys.path.insert(0, '/home/mlepekhin/Non-thematic-Text-Classification/code/allennlp_experiments')\n",
    "from data_processing import *\n",
    "from interpretation import *\n",
    "from models import *\n",
    "from training import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/mlepekhin/data'\n",
    "MODELS_DIR = '/home/mlepekhin/models'\n",
    "MODEL_ID = 'allennlp_xlm_roberta_topic_en_medium_prefix10'\n",
    "CHECKPOINTS_DIR = pathjoin(MODELS_DIR, MODEL_ID, 'checkpoints')\n",
    "BEST_MODEL = pathjoin(CHECKPOINTS_DIR, 'best.th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = 'xlm-roberta-base'\n",
    "MAX_TOKENS = 512\n",
    "all_filename = 'en_medium_all_10.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>prefix</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>at this court comes john hedges and prays to be</td>\n",
       "      <td>crime</td>\n",
       "      <td>at this court comes john hedges and prays to b...</td>\n",
       "      <td>crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>37</td>\n",
       "      <td>7</td>\n",
       "      <td>\" we are lobbying hard for statutory workplace...</td>\n",
       "      <td>education</td>\n",
       "      <td>\" we are lobbying hard for statutory workplace...</td>\n",
       "      <td>education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>each character from the format string is appen...</td>\n",
       "      <td>games</td>\n",
       "      <td>each character from the format string is appen...</td>\n",
       "      <td>games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>dont got buying stuff from billy bobs corner m...</td>\n",
       "      <td>music</td>\n",
       "      <td>dont got buying stuff from billy bobs corner m...</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>but be warned , the efficient design with mini...</td>\n",
       "      <td>business</td>\n",
       "      <td>but be warned, the efficient design with minim...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Unnamed: 0.1  \\\n",
       "521          21             1   \n",
       "737          37             7   \n",
       "740          40             0   \n",
       "660          60             0   \n",
       "411          11             1   \n",
       "\n",
       "                                                prefix     target  \\\n",
       "521    at this court comes john hedges and prays to be      crime   \n",
       "737  \" we are lobbying hard for statutory workplace...  education   \n",
       "740  each character from the format string is appen...      games   \n",
       "660  dont got buying stuff from billy bobs corner m...      music   \n",
       "411  but be warned , the efficient design with mini...   business   \n",
       "\n",
       "                                                  text      topic  \n",
       "521  at this court comes john hedges and prays to b...      crime  \n",
       "737  \" we are lobbying hard for statutory workplace...  education  \n",
       "740  each character from the format string is appen...      games  \n",
       "660  dont got buying stuff from billy bobs corner m...      music  \n",
       "411  but be warned, the efficient design with minim...   business  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = pd.read_csv(f'/home/mlepekhin/ru-gpts/en_medium_generated_texts/{all_filename}')\n",
    "all_df = all_df.sample(frac=1, random_state=42)\n",
    "all_df.target = all_df.topic\n",
    "all_df.iloc[:all_df.shape[0] // 2, :].to_csv(f'train_{all_filename}')\n",
    "all_df.iloc[all_df.shape[0] // 2:, :].to_csv(f'test_{all_filename}')\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
      " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
      " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
      " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
      " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n"
     ]
    }
   ],
   "source": [
    "all_topics = np.unique(pd.read_csv(pathjoin(DATA_DIR, 'awd_ru_topics100.csv')).topic.values)\n",
    "#all_labels = np.unique(pd.read_csv('/home/mlepekhin/data/smart_genre/ru/all.csv').target.values)\n",
    "print(all_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_csv('/home/mlepekhin/data/smart_genre/ru/all.csv').dropna().sample(random_state=42, frac=0.1).to_csv('smart_genre_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:02:39|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "01272021 11:02:39|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "01272021 11:02:40|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "01272021 11:02:41|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "01272021 11:02:41|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "01272021 11:02:42|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "01272021 11:02:44|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "01272021 11:02:44|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "01272021 11:02:45|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "01272021 11:02:48|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "01272021 11:02:48|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "01272021 11:02:49|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "01272021 11:02:51|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "01272021 11:02:51|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "01272021 11:02:51|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "01272021 11:02:53|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "01272021 11:02:53|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "01272021 11:02:53|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "01272021 11:02:55|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "01272021 11:02:55|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:02:56|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "01272021 11:02:58|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "01272021 11:02:58|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "01272021 11:03:01|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "<class 'data_processing.ClassificationDatasetReader'> train_en_medium_all_10.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e9ffc07a09443db159a1f2066dd964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65b885e0017455d9a3d226aa4ed444f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:03:19|INFO|allennlp.data.vocabulary| Fitting token dictionary from dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building the vocabulary\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e06166736d4971b990d8f4852d6542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset_reader = build_transformer_dataset_reader(transformer_model, lower=True)\n",
    "val_dataset_reader = build_transformer_dataset_reader(transformer_model, lower=True)\n",
    "\n",
    "train_data, dev_data = read_data(\n",
    "    f'train_{all_filename}', \n",
    "    f'test_{all_filename}',\n",
    "    train_dataset_reader, \n",
    "    val_dataset_reader\n",
    ")\n",
    "\n",
    "vocab = build_vocab(train_data + dev_data)\n",
    "\n",
    "train_data.index_with(vocab)\n",
    "dev_data.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:03:20|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "01272021 11:03:20|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "01272021 11:03:20|INFO|transformers.modeling_utils| loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /home/mlepekhin/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267\n",
      "01272021 11:03:33|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "01272021 11:03:33|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "01272021 11:03:34|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "01272021 11:03:35|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "01272021 11:03:35|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "01272021 11:03:37|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "model = build_pool_transformer_model(vocab, transformer_model)\n",
    "if torch.cuda.is_available():\n",
    "    cuda_device = 0\n",
    "    model = model.cuda(cuda_device)\n",
    "else:\n",
    "    cuda_device = -1\n",
    "print(cuda_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {CHECKPOINTS_DIR}\n",
    "!mkdir -p {CHECKPOINTS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:03:41|INFO|allennlp.training.optimizers| Number of trainable parameters: 278051338\n",
      "01272021 11:03:41|WARNING|allennlp.training.trainer| You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n",
      "01272021 11:03:41|INFO|allennlp.training.trainer| Beginning training.\n",
      "01272021 11:03:41|INFO|allennlp.training.trainer| Epoch 0/5\n",
      "01272021 11:03:41|INFO|allennlp.training.trainer| Worker 0 memory usage MB: 5117.928\n",
      "01272021 11:03:41|INFO|allennlp.training.trainer| GPU 0 memory usage MB: 1990\n",
      "01272021 11:03:41|INFO|allennlp.training.trainer| GPU 1 memory usage MB: 7193\n",
      "01272021 11:03:41|INFO|allennlp.training.trainer| GPU 2 memory usage MB: 18818\n",
      "01272021 11:03:41|INFO|allennlp.training.trainer| GPU 3 memory usage MB: 18470\n",
      "01272021 11:03:41|INFO|allennlp.training.trainer| Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b16deb67aff42559ec03aca39e192f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:03:58|INFO|allennlp.training.trainer| Validating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d839cba8724f41c3b7b2bfb42e8bcfa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:04:04|INFO|allennlp.training.tensorboard_writer|                        Training |  Validation\n",
      "01272021 11:04:04|INFO|allennlp.training.tensorboard_writer| accuracy           |     0.124  |     0.228\n",
      "01272021 11:04:04|INFO|allennlp.training.tensorboard_writer| gpu_0_memory_MB    |  1990.000  |       N/A\n",
      "01272021 11:04:04|INFO|allennlp.training.tensorboard_writer| gpu_1_memory_MB    |  7193.000  |       N/A\n",
      "01272021 11:04:04|INFO|allennlp.training.tensorboard_writer| gpu_2_memory_MB    |  18818.000  |       N/A\n",
      "01272021 11:04:04|INFO|allennlp.training.tensorboard_writer| gpu_3_memory_MB    |  18470.000  |       N/A\n",
      "01272021 11:04:04|INFO|allennlp.training.tensorboard_writer| loss               |     2.309  |     2.203\n",
      "01272021 11:04:04|INFO|allennlp.training.tensorboard_writer| reg_loss           |     0.000  |     0.000\n",
      "01272021 11:04:04|INFO|allennlp.training.tensorboard_writer| worker_0_memory_MB |  5117.928  |       N/A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:04:08|INFO|allennlp.training.checkpointer| Best validation performance so far. Copying weights to '/home/mlepekhin/models/allennlp_xlm_roberta_topic_en_medium_prefix10/checkpoints/best.th'.\n",
      "01272021 11:04:09|INFO|allennlp.training.trainer| Epoch duration: 0:00:27.789787\n",
      "01272021 11:04:09|INFO|allennlp.training.trainer| Estimated training time remaining: 0:02:18\n",
      "01272021 11:04:09|INFO|allennlp.training.trainer| Epoch 1/5\n",
      "01272021 11:04:09|INFO|allennlp.training.trainer| Worker 0 memory usage MB: 5117.928\n",
      "01272021 11:04:09|INFO|allennlp.training.trainer| GPU 0 memory usage MB: 18056\n",
      "01272021 11:04:09|INFO|allennlp.training.trainer| GPU 1 memory usage MB: 7193\n",
      "01272021 11:04:09|INFO|allennlp.training.trainer| GPU 2 memory usage MB: 11\n",
      "01272021 11:04:09|INFO|allennlp.training.trainer| GPU 3 memory usage MB: 18470\n",
      "01272021 11:04:09|INFO|allennlp.training.trainer| Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6154de4b2b43f68f5c502bfc98fd82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:04:26|INFO|allennlp.training.trainer| Validating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a15ceaf39c41b29eb26a89757b92f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:04:31|INFO|allennlp.training.tensorboard_writer|                        Training |  Validation\n",
      "01272021 11:04:31|INFO|allennlp.training.tensorboard_writer| accuracy           |     0.390  |     0.614\n",
      "01272021 11:04:31|INFO|allennlp.training.tensorboard_writer| gpu_0_memory_MB    |  18056.000  |       N/A\n",
      "01272021 11:04:31|INFO|allennlp.training.tensorboard_writer| gpu_1_memory_MB    |  7193.000  |       N/A\n",
      "01272021 11:04:31|INFO|allennlp.training.tensorboard_writer| gpu_2_memory_MB    |    11.000  |       N/A\n",
      "01272021 11:04:31|INFO|allennlp.training.tensorboard_writer| gpu_3_memory_MB    |  18470.000  |       N/A\n",
      "01272021 11:04:31|INFO|allennlp.training.tensorboard_writer| loss               |     1.891  |     1.349\n",
      "01272021 11:04:31|INFO|allennlp.training.tensorboard_writer| reg_loss           |     0.000  |     0.000\n",
      "01272021 11:04:31|INFO|allennlp.training.tensorboard_writer| worker_0_memory_MB |  5117.928  |       N/A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:04:35|INFO|allennlp.training.checkpointer| Best validation performance so far. Copying weights to '/home/mlepekhin/models/allennlp_xlm_roberta_topic_en_medium_prefix10/checkpoints/best.th'.\n",
      "01272021 11:04:46|INFO|allennlp.training.trainer| Epoch duration: 0:00:37.100209\n",
      "01272021 11:04:46|INFO|allennlp.training.trainer| Estimated training time remaining: 0:02:09\n",
      "01272021 11:04:46|INFO|allennlp.training.trainer| Epoch 2/5\n",
      "01272021 11:04:46|INFO|allennlp.training.trainer| Worker 0 memory usage MB: 5117.928\n",
      "01272021 11:04:46|INFO|allennlp.training.trainer| GPU 0 memory usage MB: 18056\n",
      "01272021 11:04:46|INFO|allennlp.training.trainer| GPU 1 memory usage MB: 7193\n",
      "01272021 11:04:46|INFO|allennlp.training.trainer| GPU 2 memory usage MB: 11\n",
      "01272021 11:04:46|INFO|allennlp.training.trainer| GPU 3 memory usage MB: 18470\n",
      "01272021 11:04:46|INFO|allennlp.training.trainer| Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2351b5a7b2e492a92c9e9d0af012642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:05:03|INFO|allennlp.training.trainer| Validating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b66f0154b54b90b648a02d1154d949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:05:08|INFO|allennlp.training.tensorboard_writer|                        Training |  Validation\n",
      "01272021 11:05:08|INFO|allennlp.training.tensorboard_writer| accuracy           |     0.838  |     0.950\n",
      "01272021 11:05:08|INFO|allennlp.training.tensorboard_writer| gpu_0_memory_MB    |  18056.000  |       N/A\n",
      "01272021 11:05:08|INFO|allennlp.training.tensorboard_writer| gpu_1_memory_MB    |  7193.000  |       N/A\n",
      "01272021 11:05:08|INFO|allennlp.training.tensorboard_writer| gpu_2_memory_MB    |    11.000  |       N/A\n",
      "01272021 11:05:08|INFO|allennlp.training.tensorboard_writer| gpu_3_memory_MB    |  18470.000  |       N/A\n",
      "01272021 11:05:08|INFO|allennlp.training.tensorboard_writer| loss               |     0.710  |     0.245\n",
      "01272021 11:05:08|INFO|allennlp.training.tensorboard_writer| reg_loss           |     0.000  |     0.000\n",
      "01272021 11:05:08|INFO|allennlp.training.tensorboard_writer| worker_0_memory_MB |  5117.928  |       N/A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:05:12|INFO|allennlp.training.checkpointer| Best validation performance so far. Copying weights to '/home/mlepekhin/models/allennlp_xlm_roberta_topic_en_medium_prefix10/checkpoints/best.th'.\n",
      "01272021 11:05:25|INFO|allennlp.training.trainer| Epoch duration: 0:00:39.201663\n",
      "01272021 11:05:25|INFO|allennlp.training.trainer| Estimated training time remaining: 0:01:44\n",
      "01272021 11:05:25|INFO|allennlp.training.trainer| Epoch 3/5\n",
      "01272021 11:05:25|INFO|allennlp.training.trainer| Worker 0 memory usage MB: 5117.928\n",
      "01272021 11:05:26|INFO|allennlp.training.trainer| GPU 0 memory usage MB: 18056\n",
      "01272021 11:05:26|INFO|allennlp.training.trainer| GPU 1 memory usage MB: 7193\n",
      "01272021 11:05:26|INFO|allennlp.training.trainer| GPU 2 memory usage MB: 11\n",
      "01272021 11:05:26|INFO|allennlp.training.trainer| GPU 3 memory usage MB: 18470\n",
      "01272021 11:05:26|INFO|allennlp.training.trainer| Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c1c44bd6c94d829d29485be1ea31c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:05:42|INFO|allennlp.training.trainer| Validating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2123789130ea4c0c80c7882bffa11d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:05:48|INFO|allennlp.training.tensorboard_writer|                        Training |  Validation\n",
      "01272021 11:05:48|INFO|allennlp.training.tensorboard_writer| accuracy           |     0.982  |     0.970\n",
      "01272021 11:05:48|INFO|allennlp.training.tensorboard_writer| gpu_0_memory_MB    |  18056.000  |       N/A\n",
      "01272021 11:05:48|INFO|allennlp.training.tensorboard_writer| gpu_1_memory_MB    |  7193.000  |       N/A\n",
      "01272021 11:05:48|INFO|allennlp.training.tensorboard_writer| gpu_2_memory_MB    |    11.000  |       N/A\n",
      "01272021 11:05:48|INFO|allennlp.training.tensorboard_writer| gpu_3_memory_MB    |  18470.000  |       N/A\n",
      "01272021 11:05:48|INFO|allennlp.training.tensorboard_writer| loss               |     0.150  |     0.113\n",
      "01272021 11:05:48|INFO|allennlp.training.tensorboard_writer| reg_loss           |     0.000  |     0.000\n",
      "01272021 11:05:48|INFO|allennlp.training.tensorboard_writer| worker_0_memory_MB |  5117.928  |       N/A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:05:52|INFO|allennlp.training.checkpointer| Best validation performance so far. Copying weights to '/home/mlepekhin/models/allennlp_xlm_roberta_topic_en_medium_prefix10/checkpoints/best.th'.\n",
      "01272021 11:06:03|INFO|allennlp.training.trainer| Epoch duration: 0:00:37.829512\n",
      "01272021 11:06:03|INFO|allennlp.training.trainer| Estimated training time remaining: 0:01:10\n",
      "01272021 11:06:03|INFO|allennlp.training.trainer| Epoch 4/5\n",
      "01272021 11:06:03|INFO|allennlp.training.trainer| Worker 0 memory usage MB: 5117.928\n",
      "01272021 11:06:03|INFO|allennlp.training.trainer| GPU 0 memory usage MB: 18056\n",
      "01272021 11:06:03|INFO|allennlp.training.trainer| GPU 1 memory usage MB: 7193\n",
      "01272021 11:06:03|INFO|allennlp.training.trainer| GPU 2 memory usage MB: 15216\n",
      "01272021 11:06:03|INFO|allennlp.training.trainer| GPU 3 memory usage MB: 11\n",
      "01272021 11:06:03|INFO|allennlp.training.trainer| Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72108a30c1ac4b36973b075ea3f28b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:06:20|INFO|allennlp.training.trainer| Validating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79b81fda8c049a3a0ec97f19e0cfe2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:06:26|INFO|allennlp.training.tensorboard_writer|                        Training |  Validation\n",
      "01272021 11:06:26|INFO|allennlp.training.tensorboard_writer| accuracy           |     0.994  |     0.994\n",
      "01272021 11:06:26|INFO|allennlp.training.tensorboard_writer| gpu_0_memory_MB    |  18056.000  |       N/A\n",
      "01272021 11:06:26|INFO|allennlp.training.tensorboard_writer| gpu_1_memory_MB    |  7193.000  |       N/A\n",
      "01272021 11:06:26|INFO|allennlp.training.tensorboard_writer| gpu_2_memory_MB    |  15216.000  |       N/A\n",
      "01272021 11:06:26|INFO|allennlp.training.tensorboard_writer| gpu_3_memory_MB    |    11.000  |       N/A\n",
      "01272021 11:06:26|INFO|allennlp.training.tensorboard_writer| loss               |     0.052  |     0.038\n",
      "01272021 11:06:26|INFO|allennlp.training.tensorboard_writer| reg_loss           |     0.000  |     0.000\n",
      "01272021 11:06:26|INFO|allennlp.training.tensorboard_writer| worker_0_memory_MB |  5117.928  |       N/A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:06:30|INFO|allennlp.training.checkpointer| Best validation performance so far. Copying weights to '/home/mlepekhin/models/allennlp_xlm_roberta_topic_en_medium_prefix10/checkpoints/best.th'.\n",
      "01272021 11:06:46|INFO|allennlp.training.trainer| Epoch duration: 0:00:43.143181\n",
      "01272021 11:06:46|INFO|allennlp.training.trainer| Estimated training time remaining: 0:00:37\n",
      "01272021 11:06:46|INFO|allennlp.training.trainer| Epoch 5/5\n",
      "01272021 11:06:46|INFO|allennlp.training.trainer| Worker 0 memory usage MB: 5117.928\n",
      "01272021 11:06:47|INFO|allennlp.training.trainer| GPU 0 memory usage MB: 18056\n",
      "01272021 11:06:47|INFO|allennlp.training.trainer| GPU 1 memory usage MB: 7193\n",
      "01272021 11:06:47|INFO|allennlp.training.trainer| GPU 2 memory usage MB: 23804\n",
      "01272021 11:06:47|INFO|allennlp.training.trainer| GPU 3 memory usage MB: 11\n",
      "01272021 11:06:47|INFO|allennlp.training.trainer| Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dcce2a4bdb64297bfb7d8938f2c31bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:07:03|INFO|allennlp.training.trainer| Validating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0cce274d112499fa6167504ab51b4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:07:09|INFO|allennlp.training.tensorboard_writer|                        Training |  Validation\n",
      "01272021 11:07:09|INFO|allennlp.training.tensorboard_writer| accuracy           |     1.000  |     1.000\n",
      "01272021 11:07:09|INFO|allennlp.training.tensorboard_writer| gpu_0_memory_MB    |  18056.000  |       N/A\n",
      "01272021 11:07:09|INFO|allennlp.training.tensorboard_writer| gpu_1_memory_MB    |  7193.000  |       N/A\n",
      "01272021 11:07:09|INFO|allennlp.training.tensorboard_writer| gpu_2_memory_MB    |  23804.000  |       N/A\n",
      "01272021 11:07:09|INFO|allennlp.training.tensorboard_writer| gpu_3_memory_MB    |    11.000  |       N/A\n",
      "01272021 11:07:09|INFO|allennlp.training.tensorboard_writer| loss               |     0.020  |     0.014\n",
      "01272021 11:07:09|INFO|allennlp.training.tensorboard_writer| reg_loss           |     0.000  |     0.000\n",
      "01272021 11:07:09|INFO|allennlp.training.tensorboard_writer| worker_0_memory_MB |  5117.928  |       N/A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:07:13|INFO|allennlp.training.checkpointer| Best validation performance so far. Copying weights to '/home/mlepekhin/models/allennlp_xlm_roberta_topic_en_medium_prefix10/checkpoints/best.th'.\n",
      "01272021 11:07:29|INFO|allennlp.training.trainer| Epoch duration: 0:00:42.410985\n",
      "01272021 11:07:29|INFO|allennlp.training.checkpointer| loading best weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "train_loader, dev_loader = build_data_loaders(train_data, dev_data)\n",
    "\n",
    "# You obviously won't want to create a temporary file for your training\n",
    "# results, but for execution in binder for this course, we need to do this.\n",
    "\n",
    "trainer = build_classifier_trainer(\n",
    "    model,\n",
    "    pathjoin(MODELS_DIR, MODEL_ID, 'checkpoints'),\n",
    "    train_loader,\n",
    "    dev_loader,\n",
    "    6,\n",
    "    cuda_device=cuda_device\n",
    ")\n",
    "print(\"Starting training\")\n",
    "trainer.train()\n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01272021 11:07:32|INFO|filelock| Lock 140259884749920 acquired on /home/mlepekhin/models/allennlp_xlm_roberta_topic_en_medium_prefix10/vocab/.lock\n",
      "01272021 11:07:32|INFO|filelock| Lock 140259884749920 released on /home/mlepekhin/models/allennlp_xlm_roberta_topic_en_medium_prefix10/vocab/.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1,1G\r\n",
      "-rw-rw-r-- 1 mlepekhin mlepekhin 1,1G  27 11:07 best.th\r\n",
      "drwxrwxr-x 4 mlepekhin mlepekhin 4,0K  27 11:03 log\r\n"
     ]
    }
   ],
   "source": [
    "!rm \"{CHECKPOINTS_DIR}\"/*.json \"{CHECKPOINTS_DIR}\"/model* \"{CHECKPOINTS_DIR}\"/training*\n",
    "model.vocab.save_to_files(pathjoin(MODELS_DIR, MODEL_ID, 'vocab'))\n",
    "!ls -lh \"{CHECKPOINTS_DIR}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
