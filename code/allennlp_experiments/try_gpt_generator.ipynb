{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install pytorch_pretrained_bert\n",
    "#!pip3 install gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors import Predictor\n",
    "import numpy as np\n",
    "from pytorch_pretrained_bert.tokenization_gpt2 import GPT2Tokenizer\n",
    "from pytorch_pretrained_bert.modeling_gpt2 import GPT2LMHeadModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_MODEL = 'gpt2'\n",
    "MEDIUM_MODEL = 'https://storage.googleapis.com/allennlp/models/gpt2-345M-dump'\n",
    "#MEDIUM_MODEL = 'https://drive.google.com/file/d/1hp21DmAoeq6tKoUGLEK8NtPRJVWdz_dH/view?usp=sharing'\n",
    "\n",
    "class Gpt2Predictor(Predictor):\n",
    "    \"\"\"\n",
    "    The HuggingFace implementation of GPT-2 is not an AllenNLP model;\n",
    "    however, our demo only expects an AllenNLP ``Predictor``. Accordingly,\n",
    "    we implement a ``Predictor`` that wraps the HuggingFace GPT-2 implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model_name: str = MEDIUM_MODEL,\n",
    "                 cache_size: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Each cache element is about 8MB, so size accordingly.\n",
    "        \"\"\"\n",
    "        # Cache stores tuples, so default value is a tuple\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "        # The end of text marker.\n",
    "        self.END_OF_TEXT = self.tokenizer.encoder[\"<|endoftext|>\"]\n",
    "\n",
    "\n",
    "    def predict_json(self, inputs: dict) -> dict:\n",
    "        previous_str = inputs[\"previous\"]\n",
    "        next_str = inputs.get(\"next\")\n",
    "        topk = inputs.get(\"topk\", 10)\n",
    "\n",
    "        logits = self._predict(previous_str, next_str)\n",
    "        probabilities = torch.nn.functional.softmax(logits)\n",
    "\n",
    "        best_logits, best_indices = logits.topk(topk)\n",
    "        best_words = [self.tokenizer.decode([idx.item()])\n",
    "                      for idx in best_indices]\n",
    "        best_probabilities = probabilities[best_indices].tolist()\n",
    "\n",
    "        return {\n",
    "            \"logits\": best_logits.tolist(),\n",
    "            \"probabilities\": best_probabilities,\n",
    "            \"words\": best_words,\n",
    "            \"output\": previous_str + (next_str or \"\")\n",
    "        }\n",
    "\n",
    "    def _predict(self, previous: str, next: str = None) -> torch.Tensor:\n",
    "\n",
    "        past_logits, past = (None, None)\n",
    "\n",
    "        # CASE 1: Previously seen input, no next\n",
    "        if next is None and past is not None:\n",
    "            return past_logits\n",
    "\n",
    "        # CASE 2: Previously seen input, yes next\n",
    "        elif past is not None:\n",
    "            token_ids = self.tokenizer.encode(next)\n",
    "        # CASE 3: Brand new input, no next\n",
    "        elif next is None:\n",
    "            token_ids = self.tokenizer.encode(previous)\n",
    "        # CASE 4: Brand new input, yes next\n",
    "        else:\n",
    "            token_ids = self.tokenizer.encode(previous) + self.tokenizer.encode(next)\n",
    "\n",
    "        inputs = torch.LongTensor([token_ids])\n",
    "\n",
    "        logits, present = self.model(inputs, past=past)\n",
    "        logits = logits[0, -1]\n",
    "\n",
    "        key = previous if next is None else previous + next\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def __getitem__(self, index: int) -> str:\n",
    "        return self.tokenizer.decode([index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gpt2\n",
    "predictor = Gpt2Predictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text_begin, num_of_tokens=100):\n",
    "    result = text_begin[:]\n",
    "    for token_id in range(num_of_tokens):\n",
    "        model_out = predictor.predict_json({\"previous\": result})\n",
    "        next_token = np.random.choice(model_out['words'], p=model_out['probabilities'] / np.sum(model_out['probabilities']))\n",
    "        if next_token == '\\n':\n",
    "            break\n",
    "        result += next_token\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-6c4be9a79cac>:31: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probabilities = torch.nn.functional.softmax(logits)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hi! I am so glad you liked it. I have decided that my first novel will be a horror story called 'The Black Hole', about a girl named Tiana that lives in a black hole and her attempts to free herself and the other people who live there. It will also have a lot of supernatural elements which I have not really been able to get right yet. But hopefully I will soon!\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_text(\"Hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logits': [-54.29852294921875, -55.363868713378906, -55.39118957519531, -55.43867492675781, -56.065513610839844, -56.481689453125, -56.60101318359375, -56.847679138183594, -56.959693908691406, -57.21900177001953], 'probabilities': [0.315976083278656, 0.10888809710741043, 0.10595345497131348, 0.10103980451822281, 0.053983356803655624, 0.035605497658252716, 0.03160060569643974, 0.02469276450574398, 0.02207609824836254, 0.017033597454428673], 'words': [',', ' with', ' (', ' and', ' at', ' in', '.', ' over', ' by', ' against'], 'output': 'Toronto Raptors, who are currently tied for the league leader in wins'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
