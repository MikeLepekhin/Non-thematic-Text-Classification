{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея взята у https://www.kaggle.com/sudhirnl7/logistic-regression-tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import pymorphy2\n",
    "from allennlp.data.tokenizers import Tokenizer, PretrainedTransformerTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = 'DeepPavlov/rubert-base-cased'\n",
    "tokenizer = PretrainedTransformerTokenizer(transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1532</td>\n",
       "      <td>A8</td>\n",
       "      <td>ОАО « Нижнекамскнефтехим » ( НКНХ ) не отказыв...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>389</td>\n",
       "      <td>A11</td>\n",
       "      <td>... в ходе написания ходатайства : сделать его...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>207</td>\n",
       "      <td>A14</td>\n",
       "      <td>3.2 . Т опливо и его характеристики . 3.3 . М ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1574</td>\n",
       "      <td>A8</td>\n",
       "      <td>Президент России Дмитрий Медведев в субботу на...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>196</td>\n",
       "      <td>A16</td>\n",
       "      <td>Что заставляло человечество меняться к лучшему...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 target                                               text\n",
       "0        1532     A8  ОАО « Нижнекамскнефтехим » ( НКНХ ) не отказыв...\n",
       "1         389    A11  ... в ходе написания ходатайства : сделать его...\n",
       "2         207    A14  3.2 . Т опливо и его характеристики . 3.3 . М ...\n",
       "3        1574     A8  Президент России Дмитрий Медведев в субботу на...\n",
       "4         196    A16  Что заставляло человечество меняться к лучшему..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train = pd.read_csv('/home/mlepekhin/data/ru_train')\n",
    "raw_test = pd.read_csv('/home/mlepekhin/data/ru_test')\n",
    "raw_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize, regexp_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A8' 'A11' 'A14' 'A8' 'A16' 'A12' 'A16' 'A8' 'A4' 'A14']\n"
     ]
    }
   ],
   "source": [
    "train_clean_text = [' '.join(map(str, tokenizer.tokenize(text))) for text in raw_train.text.values]\n",
    "train_clean_label = raw_train.target.values\n",
    "print(train_clean_label[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A7' 'A17' 'A17' 'A11' 'A8' 'A8' 'A11' 'A12' 'A17' 'A11']\n"
     ]
    }
   ],
   "source": [
    "test_clean_text = [' '.join(map(str, tokenizer.tokenize(text))) for text in raw_test.text.values]\n",
    "test_clean_label = raw_test.target.values\n",
    "print(test_clean_label[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigVectorizer:\n",
    "    def __init__(self, max_word_features=5000, max_char_features=10000):\n",
    "        self.vect_word = TfidfVectorizer(\n",
    "            max_features=max_word_features, lowercase=True, analyzer='word',\n",
    "            stop_words=stopwords.words('russian'), ngram_range=(1,3),dtype=np.float32\n",
    "        )\n",
    "        self.vect_char = TfidfVectorizer(\n",
    "            max_features=max_char_features, lowercase=True, analyzer='char',\n",
    "            stop_words=stopwords.words('russian'), ngram_range=(3,6),dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        vect_word = self.vect_word.fit_transform(X)\n",
    "        vect_char = self.vect_char.fit_transform(X)\n",
    "        return sparse.hstack([vect_word, vect_char])\n",
    "       \n",
    "    def transform(self, X):\n",
    "        vect_word = self.vect_word.transform(X)\n",
    "        vect_char = self.vect_char.transform(X)\n",
    "        return sparse.hstack([vect_word, vect_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = BigVectorizer()\n",
    "train_vect = vectorizer.fit_transform(train_clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vect = vectorizer.transform(test_clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictor, X_train, X_test, y_train, y_test):\n",
    "    predicted_train = predictor.predict(X_train)\n",
    "    predicted_test = predictor.predict(X_test)\n",
    "    \n",
    "    #print('recall train', recall_score(predicted_train, y_train))\n",
    "    #print('recall test', recall_score(predicted_test, y_test))\n",
    "    print('accuracy train', accuracy_score(predicted_train, y_train))\n",
    "    print('accuracy test', accuracy_score(predicted_test, y_test))\n",
    "    #print('f1 train', f1_score(predicted_train, y_train))\n",
    "    #print('f1 test', f1_score(predicted_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_grid_search = GridSearchCV(\n",
    "#    LogisticRegression(random_state=42),# class_weight='balanced'), \n",
    "#    {'C': [0.5, 1, 1.5, 2, 2.5, 3]}\n",
    "#)\n",
    "lr_estimator = LogisticRegression(random_state=42, C=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=3, random_state=42)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_estimator.fit(train_vect, train_clean_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy train 0.9861782999308915\n",
      "accuracy test 0.7763975155279503\n"
     ]
    }
   ],
   "source": [
    "evaluate(lr_estimator, train_vect, test_vect, train_clean_label, test_clean_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os.path import join as pathjoin\n",
    "\n",
    "def save_model(predictor, vectorizer, model_dir):\n",
    "    !mkdir {model_dir}\n",
    "    with open(pathjoin(model_dir, 'predictor'), 'wb') as fout:\n",
    "        fout.write(pickle.dumps(predictor))\n",
    "    with open(pathjoin(model_dir, 'vectorizer'), 'wb') as fout:\n",
    "        fout.write(pickle.dumps(vectorizer))\n",
    "        \n",
    "def load_model(model_dir):\n",
    "    return pickle.loads(open(pathjoin(model_dir, 'predictor'), 'rb').read()),\\\n",
    "           pickle.loads(open(pathjoin(model_dir, 'vectorizer'), 'rb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘simple_lr’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "save_model(lr_estimator, vectorizer, 'simple_lr')\n",
    "new_lr, new_vectorizer = load_model('simple_lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PretrainedTransformerTokenizer(transformer_model, max_length=MAX_TOKENS-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
