{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from typing import Dict, Iterable, List, Tuple\n",
    "\n",
    "import allennlp\n",
    "import torch\n",
    "from allennlp.data import DataLoader, DatasetReader, Instance, Vocabulary\n",
    "from allennlp.data.fields import LabelField, TextField\n",
    "from allennlp.data.token_indexers import TokenIndexer, PretrainedTransformerIndexer\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, PretrainedTransformerTokenizer\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules import TextFieldEmbedder, Seq2VecEncoder\n",
    "from allennlp.modules.token_embedders import PretrainedTransformerEmbedder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.seq2vec_encoders import BertPooler\n",
    "from allennlp.nn import util\n",
    "from allennlp.training.trainer import GradientDescentTrainer, Trainer\n",
    "from allennlp.training import Checkpointer\n",
    "from allennlp.training.optimizers import AdamOptimizer\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "\n",
    "from os.path import join as pathjoin\n",
    "import pandas as pd\n",
    "from allennlp.predictors import TextClassifierPredictor\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/mlepekhin/data'\n",
    "MODELS_DIR = '/home/mlepekhin/models'\n",
    "MODEL_ID = 'allennlp_bert_base_cased'\n",
    "CHECKPOINTS_DIR = pathjoin(MODELS_DIR, MODEL_ID, 'checkpoints')\n",
    "BEST_MODEL = pathjoin(CHECKPOINTS_DIR, 'best.th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = 'bert-base-cased'\n",
    "MAX_TOKENS = 510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!head -100 '/home/mlepekhin/data/multi_train' > '{DATA_DIR}/multi_train'\n",
    "#!head -10 '/home/mlepekhin/data/multi_test' > '{DATA_DIR}/multi_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef make_small_csv(fin, fout, k):\\n    df = pd.read_csv(fin)\\n    df = df.iloc[:k, :]\\n    df.to_csv(fout)\\n    \\nmake_small_csv(pathjoin(DATA_DIR, 'en_train'), pathjoin(DATA_DIR, 'en_train_small'), 50)\\nmake_small_csv(pathjoin(DATA_DIR, 'en_test'), pathjoin(DATA_DIR, 'en_test_small'), 50)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def make_small_csv(fin, fout, k):\n",
    "    df = pd.read_csv(fin)\n",
    "    df = df.iloc[:k, :]\n",
    "    df.to_csv(fout)\n",
    "    \n",
    "make_small_csv(pathjoin(DATA_DIR, 'en_train'), pathjoin(DATA_DIR, 'en_train_small'), 50)\n",
    "make_small_csv(pathjoin(DATA_DIR, 'en_test'), pathjoin(DATA_DIR, 'en_test_small'), 50)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token_indexer = PretrainedTransformerIndexer(model_name=transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDatasetReader(DatasetReader):\n",
    "    def __init__(self,\n",
    "                 lazy: bool = False,\n",
    "                 tokenizer: Tokenizer = None,\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                 max_tokens: int = None):\n",
    "        super().__init__(lazy)\n",
    "        self.tokenizer = tokenizer or PretrainedTransformerTokenizer(transformer_model, max_length=MAX_TOKENS)\n",
    "        self.token_indexers = token_indexers or {'bert_tokens': PretrainedTransformerIndexer(transformer_model)}\n",
    "        self.max_tokens = max_tokens\n",
    "        \n",
    "    def text_to_instance(self, string: str, label: str = None) -> Instance:\n",
    "        tokens = self.tokenizer.tokenize(string)\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"text\": sentence_field}\n",
    "        if label is not None:\n",
    "            fields[\"label\"] = LabelField(label)\n",
    "        return Instance(fields)\n",
    "\n",
    "    def _read(self, file_path: str) -> Iterable[Instance]:\n",
    "        dataset_df = pd.read_csv(file_path)\n",
    "        for text, label in zip(dataset_df['text'], dataset_df['target']):\n",
    "            yield self.text_to_instance(text, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(Model):\n",
    "    def __init__(self,\n",
    "                 vocab: Vocabulary,\n",
    "                 embedder: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder):\n",
    "        super().__init__(vocab)\n",
    "        self.embedder = embedder \n",
    "        num_labels = vocab.get_vocab_size(\"labels\")\n",
    "        self.encoder = encoder\n",
    "        self.classifier = torch.nn.Linear(encoder.get_output_dim(), num_labels)\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        \n",
    "\n",
    "    def forward(self,\n",
    "                text: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor=None) -> Dict[str, torch.Tensor]:\n",
    "        # Shape: (batch_size, num_tokens, embedding_dim)\n",
    "        embedded_text = self.embedder(text)\n",
    "        #print(\"embed shape\", embedded_text.shape)\n",
    "        # Shape: (batch_size, num_tokens)\n",
    "        mask = util.get_text_field_mask(text)\n",
    "        #print(\"mask shape\", mask.shape)\n",
    "        # Shape: (batch_size, encoding_dim)\n",
    "        encoded_text = self.encoder(embedded_text, mask)\n",
    "        # Shape: (batch_size, num_labels)\n",
    "        logits = self.classifier(encoded_text)\n",
    "        # Shape: (batch_size, num_labels)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        if label is not None:\n",
    "            loss = torch.nn.functional.cross_entropy(logits, label)\n",
    "            self.accuracy(logits, label)\n",
    "            return {'loss': loss, 'probs': probs}\n",
    "        else:\n",
    "            return {'probs': probs}\n",
    "    \n",
    "    def get_metrics(self, reset: bool = True) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}\n",
    "\n",
    "\n",
    "def read_data(reader: DatasetReader, train_path: str, val_path: str) -> Tuple[Iterable[Instance], Iterable[Instance]]:\n",
    "    print(\"Reading data\")\n",
    "    training_data = reader.read(train_path)\n",
    "    validation_data = reader.read(val_path)\n",
    "    return training_data, validation_data\n",
    "\n",
    "\n",
    "def build_vocab(instances: Iterable[Instance]) -> Vocabulary:\n",
    "    print(\"Building the vocabulary\")\n",
    "    return Vocabulary.from_instances(instances)\n",
    "\n",
    "\n",
    "def build_model(vocab: Vocabulary) -> Model:\n",
    "    print(\"Building the model\")\n",
    "    vocab_size = vocab.get_vocab_size(\"tokens\")\n",
    "    #embedder = BasicTextFieldEmbedder(\n",
    "    #    {\"tokens\": Embedding(embedding_dim=10, num_embeddings=vocab_size)})\n",
    "    embedding = PretrainedTransformerEmbedder(model_name=transformer_model)\n",
    "    embedder = BasicTextFieldEmbedder(token_embedders={'bert_tokens': embedding})\n",
    "    encoder = BertPooler(transformer_model)\n",
    "    return SimpleClassifier(vocab, embedder, encoder)\n",
    "\n",
    "def build_dataset_reader() -> DatasetReader:\n",
    "    return ClassificationDatasetReader()\n",
    "\n",
    "def run_training_loop():\n",
    "    dataset_reader = build_dataset_reader()\n",
    "\n",
    "    # These are a subclass of pytorch Datasets, with some allennlp-specific\n",
    "    # functionality added.\n",
    "    train_data, dev_data = read_data(dataset_reader)\n",
    "\n",
    "    vocab = build_vocab(train_data + dev_data)\n",
    "    model = build_model(vocab)\n",
    "\n",
    "    # This is the allennlp-specific functionality in the Dataset object;\n",
    "    # we need to be able convert strings in the data to integers, and this\n",
    "    # is how we do it.\n",
    "    train_data.index_with(vocab)\n",
    "    dev_data.index_with(vocab)\n",
    "\n",
    "    # These are again a subclass of pytorch DataLoaders, with an\n",
    "    # allennlp-specific collate function, that runs our indexing and\n",
    "    # batching code.\n",
    "    train_loader, dev_loader = build_data_loaders(train_data, dev_data)\n",
    "\n",
    "    # You obviously won't want to create a temporary file for your training\n",
    "    # results, but for execution in binder for this course, we need to do this.\n",
    "    with tempfile.TemporaryDirectory() as serialization_dir:\n",
    "        trainer = build_classifier_trainer(\n",
    "            model,\n",
    "            serialization_dir,\n",
    "            train_loader,\n",
    "            dev_loader,\n",
    "            checkpointer=Checkpointer(num_serialized_models_to_keep=1)\n",
    "        )\n",
    "        print(\"Starting training\")\n",
    "        trainer.train()\n",
    "        print(\"Finished training\")\n",
    "    return trainer\n",
    "\n",
    "\n",
    "# The other `build_*` methods are things we've seen before, so they are\n",
    "# in the setup section above.\n",
    "def build_data_loaders(\n",
    "    train_data: torch.utils.data.Dataset,\n",
    "    dev_data: torch.utils.data.Dataset,\n",
    ") -> Tuple[allennlp.data.DataLoader, allennlp.data.DataLoader]:\n",
    "    # Note that DataLoader is imported from allennlp above, *not* torch.\n",
    "    # We need to get the allennlp-specific collate function, which is\n",
    "    # what actually does indexing and batching.\n",
    "    train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "    dev_loader = DataLoader(dev_data, batch_size=16, shuffle=False)\n",
    "    return train_loader, dev_loader\n",
    "\n",
    "\n",
    "def build_classifier_trainer(\n",
    "    model: Model,\n",
    "    serialization_dir: str,\n",
    "    train_loader: DataLoader,\n",
    "    dev_loader: DataLoader,\n",
    "    num_epochs: int = 1,\n",
    "    cuda_device: int = -1\n",
    ") -> Trainer:\n",
    "    parameters = [\n",
    "        [n, p]\n",
    "        for n, p in model.named_parameters() if p.requires_grad\n",
    "    ]\n",
    "    optimizer = AdamOptimizer(parameters, lr=0.00005)\n",
    "    trainer = GradientDescentTrainer(\n",
    "        model=model,\n",
    "        serialization_dir=serialization_dir,\n",
    "        data_loader=train_loader,\n",
    "        validation_data_loader=dev_loader,\n",
    "        num_epochs=num_epochs,\n",
    "        optimizer=optimizer,\n",
    "        cuda_device=cuda_device,\n",
    "        validation_metric=\"+accuracy\",\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2342d40676294447a00c2dae98295c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518d4a1a8ba740adb89aeea27659bf87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building the vocabulary\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b9d0ae30ad4a73a40a91b582a81660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1686.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#trainer = run_training_loop()\n",
    "\n",
    "dataset_reader = build_dataset_reader()\n",
    "\n",
    "train_data, dev_data = read_data(\n",
    "    dataset_reader, \n",
    "    pathjoin(DATA_DIR, \"en_train\"), \n",
    "    pathjoin(DATA_DIR, \"en_test\")\n",
    ")\n",
    "\n",
    "vocab = build_vocab(train_data + dev_data)\n",
    "\n",
    "train_data.index_with(vocab)\n",
    "dev_data.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntokenizer = PretrainedTransformerTokenizer(model_name=transformer_model, max_length=MAX_TOKENS)\\ntoken_indexer = PretrainedTransformerIndexer(model_name=transformer_model)\\n#text = \"Здравствуй, дорогой друг! Я очень рад тебя видеть. Что нового?\"\\ntext = \\'Привет! \\' * 150\\ntokens = tokenizer.tokenize(text)\\nprint(len(tokens))\\n#print(\"Transformer tokens:\", tokens)\\n#print(type(tokens))\\n\\ntext_field = TextField(tokens, {\\'bert_tokens\\': token_indexer})\\ntext_field.index(vocab)\\ntoken_tensor = text_field.as_tensor(text_field.get_padding_lengths())\\nprint(\"Transformer tensors:\", token_tensor)\\n\\nembedding = PretrainedTransformerEmbedder(model_name=transformer_model)\\n\\nembedder = BasicTextFieldEmbedder(token_embedders={\\'bert_tokens\\': embedding})\\n\\ntensor_dict = text_field.batch_tensors([token_tensor])\\nprint(\\'tensor_dict\\', tensor_dict)\\nembedded_tokens = embedder(tensor_dict)\\nprint(\"Transformer embedded tokens:\", embedded_tokens)\\n\\nembedded_tokens.shape\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "tokenizer = PretrainedTransformerTokenizer(model_name=transformer_model, max_length=MAX_TOKENS)\n",
    "token_indexer = PretrainedTransformerIndexer(model_name=transformer_model)\n",
    "#text = \"Здравствуй, дорогой друг! Я очень рад тебя видеть. Что нового?\"\n",
    "text = 'Привет! ' * 150\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(len(tokens))\n",
    "#print(\"Transformer tokens:\", tokens)\n",
    "#print(type(tokens))\n",
    "\n",
    "text_field = TextField(tokens, {'bert_tokens': token_indexer})\n",
    "text_field.index(vocab)\n",
    "token_tensor = text_field.as_tensor(text_field.get_padding_lengths())\n",
    "print(\"Transformer tensors:\", token_tensor)\n",
    "\n",
    "embedding = PretrainedTransformerEmbedder(model_name=transformer_model)\n",
    "\n",
    "embedder = BasicTextFieldEmbedder(token_embedders={'bert_tokens': embedding})\n",
    "\n",
    "tensor_dict = text_field.batch_tensors([token_tensor])\n",
    "print('tensor_dict', tensor_dict)\n",
    "embedded_tokens = embedder(tensor_dict)\n",
    "print(\"Transformer embedded tokens:\", embedded_tokens)\n",
    "\n",
    "embedded_tokens.shape\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    cuda_device = 0\n",
    "    model = model.cuda(cuda_device)\n",
    "else:\n",
    "    cuda_device = -1\n",
    "print(cuda_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {CHECKPOINTS_DIR}\n",
    "!mkdir {CHECKPOINTS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb17ce9caada41a2979f9a6f93fc7841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00fd91581414a078bd0be3dafc20005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=27.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fededf2517524292bf21157432e1211f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c3e3774ec34e0ba238ce30ac412ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=27.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c0029ee1204b889499b3fb88abf428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ea9fd7a5f84e7ba96b256e48eff29a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=27.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560cb582860247f099dd1b18f4230ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36242b2180c540e0b8f5c531b98d7f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=27.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91279382a3ee41f7bcafb8f4eebf722e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8884351ac14ac7a58121f31e7bf6aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=27.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa83b5fd778431e990767556d83329f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee8078034c04690af6bf7083d7af5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=27.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0220adac80f42a0b12e5d8f973a748c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee890d242c8348cdb9775ed64d371845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=27.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d371e5f392b457da63c6a14204d7ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eefa2ce7f3824a929137647a59df6fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=27.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1f30f57e44483d8fe3d4e8ee10746c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8cc00e35a64731b49cf063124f829a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=27.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d730c4ba78bf4da38d107f7663de5607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e6c1402efc419e97d61e6ba0f9d471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=27.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "train_loader, dev_loader = build_data_loaders(train_data, dev_data)\n",
    "\n",
    "# You obviously won't want to create a temporary file for your training\n",
    "# results, but for execution in binder for this course, we need to do this.\n",
    "\n",
    "trainer = build_classifier_trainer(\n",
    "    model,\n",
    "    pathjoin(MODELS_DIR, MODEL_ID, 'checkpoints'),\n",
    "    train_loader,\n",
    "    dev_loader,\n",
    "    10,\n",
    "    cuda_device=cuda_device\n",
    ")\n",
    "print(\"Starting training\")\n",
    "trainer.train()\n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/home/mlepekhin/models/allennlp_bert_base_cased/checkpoints/*.json': No such file or directory\n",
      "rm: cannot remove '/home/mlepekhin/models/allennlp_bert_base_cased/checkpoints/model*': No such file or directory\n",
      "rm: cannot remove '/home/mlepekhin/models/allennlp_bert_base_cased/checkpoints/training*': No such file or directory\n",
      "total 416M\n",
      "-rw-rw-r-- 1 mlepekhin mlepekhin 416M авг 13 12:50 best.th\n",
      "drwxrwxr-x 4 mlepekhin mlepekhin 4,0K авг 13 12:42 log\n"
     ]
    }
   ],
   "source": [
    "!rm \"{CHECKPOINTS_DIR}\"/*.json \"{CHECKPOINTS_DIR}\"/model* \"{CHECKPOINTS_DIR}\"/training*\n",
    "!ls -lh \"{CHECKPOINTS_DIR}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'A1',\n",
       " 1: 'A12',\n",
       " 2: 'A7',\n",
       " 3: 'A16',\n",
       " 4: 'A8',\n",
       " 5: 'A22',\n",
       " 6: 'A4',\n",
       " 7: 'A11',\n",
       " 8: 'A14',\n",
       " 9: 'A9',\n",
       " 10: 'A17'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_index_to_token_vocabulary('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(BEST_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:vocabulary serialization directory /home/mlepekhin/models/allennlp_bert_base_cased/vocab is not empty\n"
     ]
    }
   ],
   "source": [
    "vocab.save_to_files(pathjoin(MODELS_DIR, MODEL_ID, 'vocab'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A1', 'A12', 'A7', 'A16', 'A8', 'A22', 'A4', 'A11', 'A14', 'A9', 'A17']\n"
     ]
    }
   ],
   "source": [
    "id_to_label = []\n",
    "with open(pathjoin(MODELS_DIR, MODEL_ID, 'vocab', 'labels.txt')) as vocab_in:\n",
    "    for line in vocab_in:\n",
    "        id_to_label.append(line.strip())\n",
    "print(id_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classes(sentence_list):\n",
    "    predictor = TextClassifierPredictor(model, dataset_reader=build_dataset_reader())\n",
    "    result = [id_to_label[np.argmax(predictor.predict(sentence)[\"probs\"])]\\\n",
    "              for sentence in sentence_list]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A1', 'A8', 'A8', 'A8']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_classes(['Здесь должно быть ваше сообщение',\n",
    "                 'Коты - лучшие домашние животные. К такому выводу пришли эксперты из издания New York Times',\n",
    "                 'It is no more than what it is.',\n",
    "                 'Жила я как-то с парнем. Я только вот на днях уволилась с работы, так как мне тяжело было работать сутки через сутки, должна была выходить на другую работу. И именно в этот период, мне сильно поплохело, начались жуткие головные боли, слабость, обмороки. Парень настоял, что нужно срочно вызывать врача. Приехала скорая, фельдшер мужик лет 50 весь седой. Позадавал вопросы мне, где болит, как болит, и зачем болит? Кто такая вообще по жизни, и чем занимаюсь? Смерил давление, температуру, написал что-то в своих бумагах, и дав лишь рекомендацию: \"больше отдыхайте, пейте воду, гуляйте на свежем воздухе\" пошёл на выход.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the time to interpret our simple classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.interpret.saliency_interpreters import SmoothGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TextClassifierPredictor(model, dataset_reader=build_dataset_reader())\n",
    "smooth_grad_interpr = SmoothGradient(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_grad_interpr.saliency_interpret_from_json({'sentence': 'Жила я как-то с парнем. Я только вот на днях уволилась с работы, так как мне тяжело было работать сутки через сутки, должна была выходить на другую работу. И именно в этот период, мне сильно поплохело, начались жуткие головные боли, слабость, обмороки. Парень настоял, что нужно срочно вызывать врача. Приехала скорая, фельдшер мужик лет 50 весь седой. Позадавал вопросы мне, где болит, как болит, и зачем болит? Кто такая вообще по жизни, и чем занимаюсь? Смерил давление, температуру, написал что-то в своих бумагах, и дав лишь рекомендацию: \"больше отдыхайте, пейте воду, гуляйте на свежем воздухе\" пошёл на выход.'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
