{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pathjoin\n",
    "from data_processing import *\n",
    "from interpretation import *\n",
    "from models import *\n",
    "from training import *\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/mlepekhin/data'\n",
    "MODELS_DIR = '/home/mlepekhin/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = 'DeepPavlov/rubert-base-cased'\n",
    "MAX_TOKENS = 512\n",
    "\n",
    "BERT_MODEL_ID = 'allennlp_rubert'\n",
    "CNN_MODEL_ID = 'allennlp_simple_cnn_ru'\n",
    "LSTM_MODEL_ID = 'allennlp_simple_lstm_ru'\n",
    "\n",
    "BERT_BEST_MODEL = pathjoin(MODELS_DIR, BERT_MODEL_ID, 'checkpoints', 'best.th')\n",
    "CNN_BEST_MODEL = pathjoin(MODELS_DIR, CNN_MODEL_ID, 'checkpoints', 'best.th')\n",
    "LSTM_BEST_MODEL = pathjoin(MODELS_DIR, LSTM_MODEL_ID, 'checkpoints', 'best.th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary().from_files(pathjoin(MODELS_DIR, BERT_MODEL_ID, 'vocab'))\n",
    "model = build_transformer_model(vocab, transformer_model)\n",
    "model.load_state_dict(torch.load(BERT_BEST_MODEL))\n",
    "\n",
    "bert_predictor = TextClassifierPredictor(\n",
    "    model, \n",
    "    dataset_reader=build_transformer_dataset_reader(transformer_model, lower=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model\n"
     ]
    }
   ],
   "source": [
    "cnn_vocab = Vocabulary().from_files(pathjoin(MODELS_DIR, CNN_MODEL_ID, 'vocab'))\n",
    "cnn_model = build_simple_cnn_model(\n",
    "    cnn_vocab, emb_size=256, output_dim=128, num_filters=32, ngram_filter_sizes=(2, 3, 4, 5)\n",
    ")\n",
    "cnn_model.load_state_dict(torch.load(CNN_BEST_MODEL))\n",
    "\n",
    "cnn_predictor = TextClassifierPredictor(\n",
    "    cnn_model, \n",
    "    dataset_reader=build_dataset_reader(None, lower=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>726</td>\n",
       "      <td>A7</td>\n",
       "      <td>Глава 1 Приступая к работе 1.1 Знакомство с те...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1871</td>\n",
       "      <td>A17</td>\n",
       "      <td>Kawasaki D-Tracker С недавних пор Kawasaki d-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1265</td>\n",
       "      <td>A17</td>\n",
       "      <td>По моему , вполне достойные книги , может и не...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>205</td>\n",
       "      <td>A11</td>\n",
       "      <td>Тест-драйв Lada Granta : новая надежда автогра...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>141</td>\n",
       "      <td>A8</td>\n",
       "      <td>среда , 2 декабря 2009 года , 12.33 Бумага всё...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 target                                               text\n",
       "0         726     A7  Глава 1 Приступая к работе 1.1 Знакомство с те...\n",
       "1        1871    A17  Kawasaki D-Tracker С недавних пор Kawasaki d-t...\n",
       "2        1265    A17  По моему , вполне достойные книги , может и не...\n",
       "3         205    A11  Тест-драйв Lada Granta : новая надежда автогра...\n",
       "4         141     A8  среда , 2 декабря 2009 года , 12.33 Бумага всё..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(pathjoin(DATA_DIR, 'ru_test'))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'A8', 1: 'A12', 2: 'A1', 3: 'A14', 4: 'A11', 5: 'A17', 6: 'A16', 7: 'A4', 8: 'A9', 9: 'A7'}\n",
      "{0: 'A8', 1: 'A12', 2: 'A1', 3: 'A14', 4: 'A11', 5: 'A17', 6: 'A16', 7: 'A4', 8: 'A9', 9: 'A7'}\n"
     ]
    }
   ],
   "source": [
    "index_to_token = vocab.get_index_to_token_vocabulary('labels')\n",
    "token_to_index = vocab.get_token_to_index_vocabulary('labels')\n",
    "print(index_to_token)\n",
    "print(cnn_vocab.get_index_to_token_vocabulary('labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "483it [02:02,  3.94it/s]\n"
     ]
    }
   ],
   "source": [
    "probs_bert, probs_cnn = [], []\n",
    "\n",
    "for text, target in tqdm.tqdm(zip(test_df.text.values, test_df.target.values)):\n",
    "    probs_bert.append(bert_predictor.predict(text)['probs'])\n",
    "    probs_cnn.append(cnn_predictor.predict(text)['probs'])\n",
    "probs_bert = np.array(probs_bert)\n",
    "probs_cnn = np.array(probs_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_mix = 0.5 * probs_bert + 0.5 * probs_cnn\n",
    "predicted_bert = [index_to_token[np.argmax(vec)] for vec in probs_bert]\n",
    "predicted_cnn = [index_to_token[np.argmax(vec)] for vec in probs_cnn]\n",
    "predicted_mix = [index_to_token[np.argmax(vec)] for vec in probs_mix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label (argum) f1_score 0.7080745341614907 precision 0.7402597402597403 recall 0.6785714285714286\n",
      "label (fictive) f1_score 0.7999999999999999 precision 0.8695652173913043 recall 0.7407407407407407\n",
      "label (instruct) f1_score 0.8275862068965517 precision 0.7058823529411765 recall 1.0\n",
      "label (reporting) f1_score 0.9483568075117371 precision 0.9805825242718447 recall 0.9181818181818182\n",
      "label (legal) f1_score 0.8148148148148148 precision 0.8461538461538461 recall 0.7857142857142857\n",
      "label (personal) f1_score 0.6516853932584269 precision 0.5918367346938775 recall 0.725\n",
      "label (commercial) f1_score 0.9467455621301775 precision 0.9411764705882353 recall 0.9523809523809523\n",
      "label (research) f1_score 0.8627450980392157 precision 0.8979591836734694 recall 0.8301886792452831\n",
      "label (info) f1_score 0.4680851063829786 precision 0.3333333333333333 recall 0.7857142857142857\n",
      "label (eval) f1_score 0.6582278481012658 precision 0.7647058823529411 recall 0.5777777777777777\n",
      "accuracy 0.8095238095238095\n"
     ]
    }
   ],
   "source": [
    "calc_classifier_metrics(np.array(predicted_bert), test_df.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label (argum) f1_score 0.6107784431137725 precision 0.6623376623376623 recall 0.5666666666666667\n",
      "label (fictive) f1_score 0.2926829268292683 precision 0.2608695652173913 recall 0.3333333333333333\n",
      "label (instruct) f1_score 0.6666666666666667 precision 0.5882352941176471 recall 0.7692307692307693\n",
      "label (reporting) f1_score 0.9299999999999999 precision 0.9029126213592233 recall 0.9587628865979382\n",
      "label (legal) f1_score 0.7096774193548387 precision 0.8461538461538461 recall 0.6111111111111112\n",
      "label (personal) f1_score 0.577319587628866 precision 0.5714285714285714 recall 0.5833333333333334\n",
      "label (commercial) f1_score 0.9101796407185628 precision 0.8941176470588236 recall 0.926829268292683\n",
      "label (research) f1_score 0.7526881720430106 precision 0.7142857142857143 recall 0.7954545454545454\n",
      "label (info) f1_score 0.46874999999999994 precision 0.45454545454545453 recall 0.4838709677419355\n",
      "label (eval) f1_score 0.6315789473684211 precision 0.7058823529411765 recall 0.5714285714285714\n",
      "accuracy 0.722567287784679\n"
     ]
    }
   ],
   "source": [
    "calc_classifier_metrics(np.array(predicted_cnn), test_df.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label (argum) f1_score 0.7204968944099378 precision 0.7532467532467533 recall 0.6904761904761905\n",
      "label (fictive) f1_score 0.8260869565217391 precision 0.8260869565217391 recall 0.8260869565217391\n",
      "label (instruct) f1_score 0.8000000000000002 precision 0.7058823529411765 recall 0.9230769230769231\n",
      "label (reporting) f1_score 0.9569377990430622 precision 0.970873786407767 recall 0.9433962264150944\n",
      "label (legal) f1_score 0.7586206896551724 precision 0.8461538461538461 recall 0.6875\n",
      "label (personal) f1_score 0.6956521739130435 precision 0.6530612244897959 recall 0.7441860465116279\n",
      "label (commercial) f1_score 0.9341317365269461 precision 0.9176470588235294 recall 0.9512195121951219\n",
      "label (research) f1_score 0.8712871287128713 precision 0.8979591836734694 recall 0.8461538461538461\n",
      "label (info) f1_score 0.5098039215686274 precision 0.3939393939393939 recall 0.7222222222222222\n",
      "label (eval) f1_score 0.7 precision 0.8235294117647058 recall 0.6086956521739131\n",
      "accuracy 0.8178053830227743\n"
     ]
    }
   ],
   "source": [
    "calc_classifier_metrics(np.array(predicted_mix), test_df.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os.path import join as pathjoin\n",
    "\n",
    "def save_model(predictor, vectorizer, model_dir):\n",
    "    !mkdir {model_dir}\n",
    "    with open(pathjoin(model_dir, 'predictor'), 'wb') as fout:\n",
    "        fout.write(pickle.dumps(predictor))\n",
    "    with open(pathjoin(model_dir, 'vectorizer'), 'wb') as fout:\n",
    "        fout.write(pickle.dumps(vectorizer))\n",
    "        \n",
    "def load_model(model_dir):\n",
    "    return pickle.loads(open(pathjoin(model_dir, 'predictor'), 'rb').read()),\\\n",
    "           pickle.loads(open(pathjoin(model_dir, 'vectorizer'), 'rb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "class BigVectorizer:\n",
    "    def __init__(self, max_word_features=5000, max_char_features=10000):\n",
    "        self.vect_word = TfidfVectorizer(\n",
    "            max_features=max_word_features, lowercase=True, analyzer='word',\n",
    "            stop_words=stopwords.words('russian'), ngram_range=(1,3),dtype=np.float32\n",
    "        )\n",
    "        self.vect_char = TfidfVectorizer(\n",
    "            max_features=max_char_features, lowercase=True, analyzer='char',\n",
    "            stop_words=stopwords.words('russian'), ngram_range=(3,6),dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        vect_word = self.vect_word.fit_transform(X)\n",
    "        vect_char = self.vect_char.fit_transform(X)\n",
    "        return sparse.hstack([vect_word, vect_char])\n",
    "       \n",
    "    def transform(self, X):\n",
    "        vect_word = self.vect_word.transform(X)\n",
    "        vect_char = self.vect_char.transform(X)\n",
    "        return sparse.hstack([vect_word, vect_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictor, vectorizer = load_model('simple_lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_proba = lr_predictor.predict_proba(vectorizer.transform(test_df.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_lr = lr_predictor.predict(vectorizer.transform(test_df.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label (argum) f1_score 0.7100591715976331 precision 0.7792207792207793 recall 0.6521739130434783\n",
      "label (fictive) f1_score 0.7567567567567568 precision 0.6086956521739131 recall 1.0\n",
      "label (instruct) f1_score 0.6923076923076924 precision 0.5294117647058824 recall 1.0\n",
      "label (reporting) f1_score 0.9124423963133641 precision 0.9611650485436893 recall 0.868421052631579\n",
      "label (legal) f1_score 0.9230769230769231 precision 0.9230769230769231 recall 0.9230769230769231\n",
      "label (personal) f1_score 0.6732673267326732 precision 0.6938775510204082 recall 0.6538461538461539\n",
      "label (commercial) f1_score 0.9239766081871345 precision 0.9294117647058824 recall 0.9186046511627907\n",
      "label (research) f1_score 0.8200000000000001 precision 0.8367346938775511 recall 0.803921568627451\n",
      "label (info) f1_score 0.39999999999999997 precision 0.2727272727272727 recall 0.75\n",
      "label (eval) f1_score 0.7567567567567567 precision 0.8235294117647058 recall 0.7\n",
      "accuracy 0.7971014492753623\n"
     ]
    }
   ],
   "source": [
    "calc_classifier_metrics(np.array(predicted_lr), test_df.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_index = {}\n",
    "for class_id, cl in enumerate(lr_predictor.classes_):\n",
    "    index_to_index[class_id] = token_to_index[cl]\n",
    "\n",
    "lr_proba_transformed = []\n",
    "for vec in lr_proba:\n",
    "    new_vec = np.zeros(len(vec))\n",
    "    for i in range(len(vec)):\n",
    "        new_vec[index_to_index[i]] = vec[i]\n",
    "    lr_proba_transformed.append(new_vec[:])\n",
    "lr_proba_transformed = np.array(lr_proba_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label (argum) f1_score 0.7272727272727272 precision 0.7792207792207793 recall 0.6818181818181818\n",
      "label (fictive) f1_score 0.8292682926829269 precision 0.7391304347826086 recall 0.9444444444444444\n",
      "label (instruct) f1_score 0.8387096774193549 precision 0.7647058823529411 recall 0.9285714285714286\n",
      "label (reporting) f1_score 0.9573459715639812 precision 0.9805825242718447 recall 0.9351851851851852\n",
      "label (legal) f1_score 0.8148148148148148 precision 0.8461538461538461 recall 0.7857142857142857\n",
      "label (personal) f1_score 0.7368421052631581 precision 0.7142857142857143 recall 0.7608695652173914\n",
      "label (commercial) f1_score 0.9404761904761904 precision 0.9294117647058824 recall 0.9518072289156626\n",
      "label (research) f1_score 0.8627450980392157 precision 0.8979591836734694 recall 0.8301886792452831\n",
      "label (info) f1_score 0.5306122448979591 precision 0.3939393939393939 recall 0.8125\n",
      "label (eval) f1_score 0.7532467532467532 precision 0.8529411764705882 recall 0.6744186046511628\n",
      "accuracy 0.8322981366459627\n"
     ]
    }
   ],
   "source": [
    "probs_mix_lr = 0.3 * probs_bert + 0.4 * lr_proba_transformed + 0.3 * probs_cnn\n",
    "predicted_mix_lr = [index_to_token[np.argmax(vec)] for vec in probs_mix_lr]\n",
    "calc_classifier_metrics(np.array(predicted_mix_lr), test_df.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
